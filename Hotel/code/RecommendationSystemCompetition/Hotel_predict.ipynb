{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Expedia 酒店推荐比赛\n",
    "\n",
    "[link](https://www.kaggle.com/c/expedia-hotel-recommendations/overview)\n",
    "\n",
    "### 问题背景\n",
    "![](./img/kaggle-expedia-hotel-recommendation.png)\n",
    "\n",
    "### 数据描述\n",
    "\n",
    "Expedia has provided you logs of customer behavior. These include what customers searched for, how they interacted with search results (click/book), whether or not the search result was a travel package. The data in this competition is a random selection from Expedia and is not representative of the overall statistics.\n",
    "\n",
    "Expedia is interested in predicting which hotel group a user is going to book. Expedia has in-house algorithms to form hotel clusters, where similar hotels for a search (based on historical price, customer star ratings, geographical locations relative to city center, etc) are grouped together. These hotel clusters serve as good identifiers to which types of hotels people are going to book, while avoiding outliers such as new hotels that don't have historical data.\n",
    "\n",
    "Your goal of this competition is to predict the booking outcome (hotel cluster) for a user event, based on their search and other attributes associated with that user event.\n",
    "\n",
    "The train and test datasets are split based on time: training data from 2013 and 2014, while test data are from 2015. The public/private leaderboard data are split base on time as well. Training data includes all the users in the logs, including both click events and booking events. Test data only includes booking events. \n",
    "\n",
    "destinations.csv data consists of features extracted from hotel reviews text. \n",
    "\n",
    "Note that some srch_destination_id's in the train/test files don't exist in the destinations.csv file. This is because some hotels are new and don't have enough features in the latent space. Your algorithm should be able to handle this missing information.\n",
    "\n",
    "### File descriptions\n",
    "\n",
    "* **train.csv** - the training set\n",
    "* **test.csv** - the test set\n",
    "* **destinations.csv** - hotel search latent attributes\n",
    "* **sample_submission.csv** - a sample submission file in the correct format\n",
    "\n",
    "\n",
    "### Data fields\n",
    "\n",
    "**train/test.csv**\n",
    "\n",
    "![](./img/data.png)\n",
    "\n",
    "### 评估标准与提交格式\n",
    "\n",
    "![](./img/eval.png)\n",
    "\n",
    "### 解法图示\n",
    "\n",
    "![](./img/solution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据泄露处理 data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from heapq import nlargest # 堆\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import model_selection\n",
    "# (brew install libomp)\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import h5py # 数据存储格式\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use pandas to check the dataset, memory requirement about 16G, not suggest running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv(\"./input/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter wrong data and modify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[(train[\"srch_ci\"]>\"2021-01-01 00:00:00\") & (train[\"is_booking\"] ==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[(train[\"srch_co\"]>\"2021-01-01 00:00:00\") & (train[\"is_booking\"] ==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[(train[\"srch_ci\"].isnull()) & (train[\"is_booking\"] ==1)][\"srch_ci\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[(train[\"srch_co\"].isnull()) & (train[\"is_booking\"] ==1)][\"srch_ci\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_data(train,column_list):\n",
    "#     '''\n",
    "#     data is from 2014-08-11 07:46:59 to 2014-09-18 08:52:42, if a reservation is 10 years after, it's probably wrong record.\n",
    "    \n",
    "#     '''\n",
    "#     for i in column_list:\n",
    "#         data = train[train[i]>\"2021-01-01 00:00:00\"].index\n",
    "#         train.drop(data, axis=0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_list = [\"srch_ci\",'srch_co']\n",
    "# filter_data(train,column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['srch_ci']=pd.to_datetime(train['srch_ci'])\n",
    "# train['srch_co']=pd.to_datetime(train['srch_co'])\n",
    "# train['date_time']=pd.to_datetime(train['date_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if check out is early than check in date\n",
    "# train[train[\"srch_ci\"]>train[\"srch_co\"]].loc[:,[\"date_time\",\"srch_ci\",\"srch_co\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # exchange position if check out is early than check in date\n",
    "# def exchange_in_out(data):\n",
    "#     index = data[data[\"srch_ci\"]>data[\"srch_co\"]].index\n",
    "#     data[\"tem\"] = data[\"srch_ci\"]\n",
    "#     for i in index:\n",
    "#         data[\"tem\"][i] = data.loc[i,\"srch_ci\"]\n",
    "#         data.loc[i,\"srch_ci\"] = data.loc[i,'srch_co']\n",
    "#         data.loc[i,'srch_co'] = data.loc[i,\"tem\"]\n",
    "#     data.drop([\"tem\"],axis = 1, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exchange_in_out(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[train[\"srch_ci\"]>train[\"srch_co\"]].loc[:,[\"date_time\",\"srch_ci\",\"srch_co\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv(\"./input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check out bound record\n",
    "# test[test[\"srch_co\"]>\"2021-01-01\"].loc[:,[\"date_time\",\"srch_ci\",\"srch_co\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test[test[\"srch_ci\"]>\"2021-01-01 00:00:00\"].loc[:,[\"date_time\",\"srch_ci\",\"srch_co\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.loc[312920,\"srch_ci\"] = \"2016-01-21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # No out bound record\n",
    "# test[test[\"srch_ci\"]>\"2021-01-01\"].loc[:,[\"date_time\",\"srch_ci\",\"srch_co\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if check out is early than check in date\n",
    "# test[test[\"srch_ci\"]>test[\"srch_co\"]].loc[:,[\"date_time\",\"srch_ci\",\"srch_co\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exchange_in_out(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test[test[\"srch_ci\"]>test[\"srch_co\"]].loc[:,[\"date_time\",\"srch_ci\",\"srch_co\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test['srch_ci']=pd.to_datetime(test['srch_ci'])\n",
    "# test['srch_co']=pd.to_datetime(test['srch_co'])\n",
    "# test['date_time']=pd.to_datetime(test['date_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store modified train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv('./output/train.csv',index=False)\n",
    "# test.to_csv('./output/test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !! process above needs long time and memory, not suggest running !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start process of data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# prepare matched cluster\n",
    "def cluster_weight_collect():\n",
    "    \"\"\"\n",
    "    Process the data to create 4 dict by creating different unique key.\n",
    "    Each dict contains all clusters appeared in the train.test, \n",
    "    each element is a dic mapping to its weight.\n",
    "    \n",
    "\n",
    "\n",
    "    Returns:\n",
    "        Four dict:\n",
    "        best_hotel_mainWeight   # user_id, user_location_city, srch_destination_id, hotel_country, hotel_market\n",
    "        best_hotel_secWeight    # user_id, srch_destination_id, hotel_country, hotel_market\n",
    "        best_hotels_od_ulc      # user_location_city, srch_destination_id, hotel_country, hotel_market\n",
    "        best_hotels_uid_miss    # user_location_city, srch_destination_id\n",
    "\n",
    "    \"\"\"\n",
    "    f = open(\"./output/train.csv\", \"r\")  \n",
    "    f.readline()\n",
    "    \n",
    "    best_hotel_mainWeight = dict() # user_id, user_location_city, srch_destination_id, hotel_country, hotel_market\n",
    "    best_hotel_secWeight = dict()  # user_id, srch_destination_id, hotel_country, hotel_market\n",
    "    best_hotels_od_ulc = dict()    # user_location_city, srch_destination_id, hotel_country, hotel_market\n",
    "    best_hotels_uid_miss = dict()  # user_location_city, srch_destination_id\n",
    "\n",
    "    # Calc counts\n",
    "    while True:\n",
    "        line = f.readline().strip() # strip space\n",
    "\n",
    "        if line == '':\n",
    "            print(\"Finish reading \")\n",
    "            break # \n",
    "\n",
    "        # abstract information from file\n",
    "        arr = line.split(\",\") \n",
    "        \n",
    "        book_year = int(arr[0][:4])           # year of book\n",
    "        book_month = int(arr[0][5:7])         # month of book\n",
    "        user_location_city = arr[5]           # the city of the coutomer is located \n",
    "        orig_destination_distance = arr[6]    # physical distance, null means cannnot be calculated\n",
    "        user_id = arr[7]                      # ID of user\n",
    "        srch_destination_id = arr[16]         # hotel searched Id\n",
    "        hotel_country = arr[21]               # country of the hotel\n",
    "        hotel_market = arr[22]                # maket of hotel\n",
    "        is_booking = float(arr[18])           # 1 if booking\n",
    "        hotel_cluster = arr[23]               # cluster of the hotel\n",
    " \n",
    "        # creat some values\n",
    "        # time weight by month\n",
    "        append_0 = ((book_year - 2012)*12 + (book_month - 12)) \n",
    "        # time * 2 + the weight of booking \n",
    "        append_1 = append_0 * append_0 * (3 + 17.60*is_booking)  \n",
    "        \n",
    "        #temporarily not use \n",
    "        append_2 = 3 + 5.56*is_booking \n",
    "\n",
    "        # create key： unique(user_id, user_location_city, srch_destination_id, hotel_country, hotel_market)\n",
    "        if user_location_city != '' and orig_destination_distance != '' and user_id !='' and srch_destination_id != '' and hotel_country != '':\n",
    "            # hash processing\n",
    "            solution = hash(str(user_id)+':'+str(user_location_city)+':'+str(srch_destination_id)+':'+str(hotel_country)+':'+str(hotel_market))\n",
    "            # found cluster, add weight; not found, add the cluster, give it initial weight\n",
    "            if solution in best_hotel_mainWeight:\n",
    "                if hotel_cluster in best_hotel_mainWeight[solution]:\n",
    "                    best_hotel_mainWeight[solution][hotel_cluster] += append_1\n",
    "                else:\n",
    "                    best_hotel_mainWeight[solution][hotel_cluster] = append_1\n",
    "            # if not found, create a solution and give it a weight\n",
    "            else:\n",
    "                best_hotel_mainWeight[solution] = dict()\n",
    "                best_hotel_mainWeight[solution][hotel_cluster] = append_1\n",
    "\n",
    "        # create key： unique(user_id, srch_destination_id, hotel_country, hotel_market)\n",
    "        if user_location_city != '' and orig_destination_distance != '' and user_id !='' and srch_destination_id != '':\n",
    "            solution_sec = hash(str(user_id)+':'+str(srch_destination_id)+':'+str(hotel_country)+':'+str(hotel_market))\n",
    "            # same as above \n",
    "            if solution_sec in best_hotel_secWeight:\n",
    "                if hotel_cluster in best_hotel_secWeight[solution_sec]:\n",
    "                    best_hotel_secWeight[solution_sec][hotel_cluster] += append_1\n",
    "                else:\n",
    "                    best_hotel_secWeight[solution_sec][hotel_cluster] = append_1\n",
    "            else:\n",
    "                best_hotel_secWeight[solution_sec] = dict()\n",
    "                best_hotel_secWeight[solution_sec][hotel_cluster] = append_1\n",
    "\n",
    "        # create key： unique(user_location_city, srch_destination_id, hotel_country, hotel_market)\n",
    "        if user_location_city != '' and orig_destination_distance == '' and srch_destination_id != '' and hotel_country != '':\n",
    "            solution_thr = hash(str(user_location_city)+':'+str(srch_destination_id)+':'+str(hotel_country)+':'+str(hotel_market))\n",
    "            if solution_thr in best_hotels_uid_miss:\n",
    "                if hotel_cluster in best_hotels_uid_miss[solution_thr]:\n",
    "                    best_hotels_uid_miss[solution_thr][hotel_cluster] += append_1\n",
    "                else:\n",
    "                    best_hotels_uid_miss[solution_thr][hotel_cluster] = append_1\n",
    "            else:\n",
    "                best_hotels_uid_miss[solution_thr] = dict()\n",
    "                best_hotels_uid_miss[solution_thr][hotel_cluster] = append_1\n",
    "\n",
    "        # create key： unique(user_location_city, srch_destination_id)\n",
    "        if user_location_city != '' and orig_destination_distance != '':\n",
    "            solution_fo = hash(str(user_location_city)+':'+str(orig_destination_distance))\n",
    "\n",
    "            if solution_fo in best_hotels_od_ulc:\n",
    "                if hotel_cluster in best_hotels_od_ulc[solution_fo]:\n",
    "                    best_hotels_od_ulc[solution_fo][hotel_cluster] += append_0\n",
    "                else:\n",
    "                    best_hotels_od_ulc[solution_fo][hotel_cluster] = append_0\n",
    "            else:\n",
    "                best_hotels_od_ulc[solution_fo] = dict()\n",
    "                best_hotels_od_ulc[solution_fo][hotel_cluster] = append_0\n",
    "\n",
    "    f.close()\n",
    "    return best_hotel_mainWeight,best_hotel_secWeight, best_hotels_od_ulc, best_hotels_uid_miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(best_hotel_mainWeight,best_hotel_secWeight, best_hotels_od_ulc, best_hotels_uid_miss):\n",
    "    \"\"\"\n",
    "    Generate prediction by the result weight dic from fuction cluster_weight_collect\n",
    "\n",
    "    \"\"\"\n",
    "    path = './output/match_pred.csv'\n",
    "    out = open(path, \"w\")\n",
    "    \n",
    "    # get test set and read\n",
    "    f = open(\"./output/test.csv\", \"r\")\n",
    "    f.readline()\n",
    "    total_fo = 0      # total solution_fo in the test\n",
    "    total_thr = 0      # total solution_thr in the test\n",
    "    total_sec = 0     # total solution_sec in the test\n",
    "         \n",
    "    \n",
    "    # write the first line in the result file\n",
    "    out.write(\"id,hotel_cluster\\n\")\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        line = f.readline().strip() # test set line with strip()\n",
    "        if line == '':\n",
    "            print(\"Finish reading\")\n",
    "            break\n",
    "\n",
    "        arr = line.split(\",\")\n",
    "        ID = arr[0]                            # test set id \n",
    "        user_location_city = arr[6]            # the city of the coutomer is located \n",
    "        orig_destination_distance = arr[7]     # physical distance, null means cannnot be calculated\n",
    "        user_id = arr[8]                       # ID of user\n",
    "        srch_destination_id = arr[17]          # hotel searched Id\n",
    "        hotel_country = arr[20]                # country of the hotel\n",
    "        hotel_market = arr[21]                 # maket of hotel\n",
    "         \n",
    "        out.write(str(ID) + ',')\n",
    "        filled = []\n",
    "        \n",
    "        # solution_fo is same as the usage in fuction cluster_weight_collect\n",
    "        solution_fo = hash(str(user_location_city)+':'+str(orig_destination_distance))\n",
    "        if solution_fo in best_hotels_od_ulc:\n",
    "            d = best_hotels_od_ulc[solution_fo]\n",
    "            # get the top 5 cluster\n",
    "            topitems = nlargest(5, sorted(d.items()), key=itemgetter(1))\n",
    "            for i in range(len(topitems)):\n",
    "                # choose 5 cluster, if already contains, ignore. Full, break.\n",
    "                if topitems[i][0] in filled:\n",
    "                    continue\n",
    "                if len(filled) == 5:\n",
    "                    break\n",
    "                # write the matched cluster into result file.\n",
    "                out.write(' ' + topitems[i][0])\n",
    "                filled.append(topitems[i][0])\n",
    "                total_fo += 1\n",
    "\n",
    "        if orig_destination_distance == '':\n",
    "            solution_thr = hash(str(user_location_city)+':'+str(srch_destination_id)+':'+str(hotel_country)+':'+str(hotel_market))\n",
    "            if solution_thr in best_hotels_uid_miss:\n",
    "                d = best_hotels_uid_miss[solution_thr]\n",
    "                topitems = nlargest(4, sorted(d.items()), key=itemgetter(1))\n",
    "                for i in range(len(topitems)):\n",
    "                    if topitems[i][0] in filled:\n",
    "                        continue\n",
    "                    if len(filled) == 5:\n",
    "                        break\n",
    "                    out.write(' ' + topitems[i][0])\n",
    "                    filled.append(topitems[i][0])\n",
    "                    total_thr += 1\n",
    "\n",
    "        solution = hash(str(user_id)+':'+str(user_location_city)+':'+str(srch_destination_id)+':'+str(hotel_country)+':'+str(hotel_market))\n",
    "        solution_sec = hash(str(user_id)+':'+str(srch_destination_id)+':'+str(hotel_country)+':'+str(hotel_market))\n",
    "        if solution_sec in best_hotel_secWeight and solution not in best_hotel_mainWeight:\n",
    "            d = best_hotel_secWeight[solution_sec]\n",
    "            topitems = nlargest(4, sorted(d.items()), key=itemgetter(1))\n",
    "            for i in range(len(topitems)):\n",
    "                if topitems[i][0] in filled:\n",
    "                    continue\n",
    "                if len(filled) == 5:\n",
    "                    break\n",
    "                out.write(' ' + topitems[i][0])\n",
    "                filled.append(topitems[i][0])\n",
    "                total_sec += 1\n",
    "\n",
    "        out.write(\"\\n\")\n",
    "    out.close()\n",
    "    print('Total solution_fo: {} ...'.format(total_fo))\n",
    "    print('Total solution_thr: {} ...'.format(total_thr))\n",
    "    print('Total solution_sec: {} ...'.format(total_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_hotel_mainWeight,best_hotel_secWeight, best_hotels_od_ulc, best_hotels_uid_miss = cluster_weight_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_submission(best_hotel_mainWeight,best_hotel_secWeight, best_hotels_od_ulc, best_hotels_uid_miss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model in common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_destination_id</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>hotel_market</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>1537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>1597</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006447</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65608</th>\n",
       "      <td>65098</td>\n",
       "      <td>50</td>\n",
       "      <td>688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65609</th>\n",
       "      <td>65102</td>\n",
       "      <td>50</td>\n",
       "      <td>608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65610</th>\n",
       "      <td>65103</td>\n",
       "      <td>50</td>\n",
       "      <td>608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65611</th>\n",
       "      <td>65104</td>\n",
       "      <td>50</td>\n",
       "      <td>639</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65612</th>\n",
       "      <td>65107</td>\n",
       "      <td>50</td>\n",
       "      <td>458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65613 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       srch_destination_id  hotel_country  hotel_market   0   1   2         3  \\\n",
       "0                        0            100           796 NaN NaN NaN  1.000000   \n",
       "1                        1             76          1537 NaN NaN NaN       NaN   \n",
       "2                        2             48           152 NaN NaN NaN       NaN   \n",
       "3                        3             17          1597 NaN NaN NaN       NaN   \n",
       "4                        4              7           246 NaN NaN NaN  0.006447   \n",
       "...                    ...            ...           ...  ..  ..  ..       ...   \n",
       "65608                65098             50           688 NaN NaN NaN       NaN   \n",
       "65609                65102             50           608 NaN NaN NaN       NaN   \n",
       "65610                65103             50           608 NaN NaN NaN       NaN   \n",
       "65611                65104             50           639 NaN NaN NaN       NaN   \n",
       "65612                65107             50           458 NaN NaN NaN       NaN   \n",
       "\n",
       "        4         5   6  ...        90  91  92        93        94  95  96  \\\n",
       "0     NaN       NaN NaN  ...       NaN NaN NaN       NaN       NaN NaN NaN   \n",
       "1     NaN       NaN NaN  ...       NaN NaN NaN       NaN       NaN NaN NaN   \n",
       "2     NaN  0.050847 NaN  ...       NaN NaN NaN       NaN       NaN NaN NaN   \n",
       "3     NaN       NaN NaN  ...       NaN NaN NaN       NaN       NaN NaN NaN   \n",
       "4     NaN       NaN NaN  ...  0.001235 NaN NaN  0.008642       NaN NaN NaN   \n",
       "...    ..       ...  ..  ...       ...  ..  ..       ...       ...  ..  ..   \n",
       "65608 NaN       NaN NaN  ...       NaN NaN NaN       NaN  0.666667 NaN NaN   \n",
       "65609 NaN       NaN NaN  ...       NaN NaN NaN       NaN       NaN NaN NaN   \n",
       "65610 NaN       NaN NaN  ...       NaN NaN NaN       NaN       NaN NaN NaN   \n",
       "65611 NaN  0.115385 NaN  ...       NaN NaN NaN       NaN       NaN NaN NaN   \n",
       "65612 NaN       NaN NaN  ...       NaN NaN NaN       NaN       NaN NaN NaN   \n",
       "\n",
       "       97  98        99  \n",
       "0     NaN NaN       NaN  \n",
       "1     NaN NaN       NaN  \n",
       "2     NaN NaN       NaN  \n",
       "3     NaN NaN       NaN  \n",
       "4     NaN NaN  0.014403  \n",
       "...    ..  ..       ...  \n",
       "65608 NaN NaN       NaN  \n",
       "65609 NaN NaN       NaN  \n",
       "65610 NaN NaN       NaN  \n",
       "65611 NaN NaN       NaN  \n",
       "65612 NaN NaN       NaN  \n",
       "\n",
       "[65613 rows x 103 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists('./output/srch_dest_hc_hm_agg.csv'): \n",
    "    aggMod = pd.read_csv('./output/srch_dest_hc_hm_agg.csv')\n",
    "else:\n",
    "\n",
    "    # read by chunk  chunksize\n",
    "    reader = pd.read_csv('./output/train.csv', parse_dates=['date_time', 'srch_ci', 'srch_co'], chunksize=200000)  # parse_dates \n",
    "\n",
    "    # get sum and count of agg in ['srch_destination_id','hotel_country','hotel_market','hotel_cluster'] form\n",
    "    pieces = [chunk.groupby(['srch_destination_id','hotel_country','hotel_market','hotel_cluster'])['is_booking'].agg(['sum','count']) for chunk in reader]\n",
    "    agg = pd.concat(pieces).groupby(level=[0,1,2,3]).sum()\n",
    "\n",
    "    del pieces # release memory\n",
    "    agg.dropna(inplace=True) # delete nan\n",
    "\n",
    "    # Weighted aggregation\n",
    "    agg['sum_and_cnt'] = 0.85*agg['sum'] + 0.15*agg['count'] \n",
    "\n",
    "    # partition in specific index\n",
    "    agg = agg.groupby(level=[0,1,2]).apply(lambda x: x.astype(float)/x.sum())\n",
    "    agg.reset_index(inplace=True)\n",
    "\n",
    "    # data PivotTable.\n",
    "    aggMod = agg.pivot_table(index=['srch_destination_id','hotel_country','hotel_market'], columns='hotel_cluster', values='sum_and_cnt').reset_index()\n",
    "    aggMod.to_csv('./output/srch_dest_hc_hm_agg.csv', index=False)\n",
    "    # release memory\n",
    "    del agg \n",
    "    \n",
    "destinations = pd.read_csv('./input/destinations.csv')\n",
    "submission = pd.read_csv('./input/sample_submission.csv')\n",
    "aggMod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理部分 pre_process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(data):\n",
    "    \n",
    "    # create new feature\n",
    "    # living time\n",
    "    data['srch_duration'] = data.srch_co-data.srch_ci\n",
    "#     data['srch_duration'] = pd.to_datetime(data['srch_duration'],format = '%Y-%m-%d')\n",
    "    data['srch_duration'] = data['srch_duration'].apply(lambda td: td/np.timedelta64(1, 'D')) # Datetime转天数\n",
    "    \n",
    "    # time from booing to checkin in days\n",
    "    data['time_to_ci'] = data.srch_ci-data.date_time\n",
    "    data['time_to_ci'] = data['time_to_ci'].apply(lambda td: td/np.timedelta64(1, 'D'))\n",
    "    \n",
    "    # checkin time\n",
    "    data['ci_month'] = data['srch_ci'].apply(lambda dt: dt.month)\n",
    "    data['ci_day'] = data['srch_ci'].apply(lambda dt: dt.day)\n",
    "    #data['ci_year'] = data['srch_ci'].apply(lambda dt: dt.year)\n",
    "    \n",
    "    # booking time information\n",
    "    data['bk_month'] = data['date_time'].apply(lambda dt: dt.month)\n",
    "    data['bk_day'] = data['date_time'].apply(lambda dt: dt.day)\n",
    "    #data['bk_year'] = data['date_time'].apply(lambda dt: dt.year)\n",
    "    data['bk_hour'] = data['date_time'].apply(lambda dt: dt.hour)\n",
    "    data.drop(['date_time', 'user_id', 'srch_ci', 'srch_co'], axis=1, inplace=True)\n",
    "    \n",
    "    data.fillna(0, inplace=True) # 缺失值填充， 也可以使用更合理的方式填充，平均值，众数等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training process\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=0, n_jobs=-1, warm_start=True)\n",
    "count = 0\n",
    "chunksize = 200000\n",
    "reader = pd.read_csv('./output/train.csv', parse_dates=['date_time', 'srch_ci', 'srch_co'], chunksize=chunksize)\n",
    "for chunk in reader:\n",
    "    try:\n",
    "        chunk = chunk[chunk.is_booking==1]\n",
    "        chunk = pd.merge(chunk, destinations, how='left', on='srch_destination_id') # join\n",
    "        chunk = pd.merge(chunk, aggMod, how='left', on=['srch_destination_id','hotel_country','hotel_market'])\n",
    "        pre_process(chunk) # pre-process\n",
    "        y = chunk.hotel_cluster\n",
    "        chunk.drop(['cnt', 'hotel_cluster', 'is_booking'], axis=1, inplace=True)\n",
    "        \n",
    "        # 训练\n",
    "        if len(y.unique()) == 100:\n",
    "            clf.set_params(n_estimators=clf.n_estimators+1)\n",
    "            clf.fit(chunk, y)\n",
    "        \n",
    "        count = count + chunksize\n",
    "        print('%d rows completed' % count)\n",
    "        if(count/chunksize == 300):\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print('Error: %s' % str(e))\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prediction process\n",
    "count = 0\n",
    "chunksize = 10000\n",
    "preds = np.empty((submission.shape[0],clf.n_classes_))\n",
    "reader = pd.read_csv('./output/test.csv', parse_dates=['date_time', 'srch_ci', 'srch_co'], chunksize=chunksize)\n",
    "for chunk in reader:\n",
    "    try:\n",
    "        chunk = pd.merge(chunk, destinations, how='left', on='srch_destination_id')\n",
    "        chunk = pd.merge(chunk, aggMod, how='left', on=['srch_destination_id','hotel_country','hotel_market'])\n",
    "        chunk.drop(['id'], axis=1, inplace=True)\n",
    "        pre_process(chunk)\n",
    "\n",
    "        pred = clf.predict_proba(chunk)\n",
    "        preds[count:(count + chunk.shape[0]),:] = pred\n",
    "        count = count + chunksize\n",
    "        print('%d rows completed' % count)\n",
    "    except Exception as e:\n",
    "        print('Error: %s' % str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output predict result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del clf\n",
    "print('writing current probabilities to file')\n",
    "if os.path.exists('./output/probs/allpreds.h5'):\n",
    "    with h5py.File('./output/probs/allpreds.h5', 'r+') as hf:\n",
    "            print('reading in and combining probabilities')\n",
    "            predslatesthf = hf['preds_latest']\n",
    "            preds += predslatesthf.value\n",
    "            print('writing latest probabilities to file')\n",
    "            predshf[...] = preds\n",
    "else:\n",
    "    with h5py.File('./output/probs/allpreds.h5', 'w') as hf:\n",
    "        print('writing latest probabilities to file')\n",
    "        hf.create_dataset('preds_latest', data=preds)\n",
    "\n",
    "# print('generating submission')\n",
    "# col_ind = np.argsort(-preds, axis=1)[:,:5]\n",
    "# hc = [' '.join(row.astype(str)) for row in col_ind]\n",
    "# sub = pd.DataFrame(data=hc, index=submission.id)\n",
    "# sub.reset_index(inplace=True)\n",
    "# sub.columns = submission.columns\n",
    "# sub.to_csv('./output/pred_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估标准 evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#评估标准\n",
    "def map5eval(preds, dtrain):\n",
    "    actual = dtrain.get_label() # 真实预测\n",
    "    predicted = preds.argsort(axis=1)[:,-np.arange(5)] # 排序排在前五位的酒店类别\n",
    "    metric = 0.\n",
    "    for i in range(5):\n",
    "        metric += np.sum(actual==predicted[:,i])/(i+1) # 计算\n",
    "    metric /= actual.shape[0]\n",
    "    return 'MAP@5', -metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows will be skipped and next 1000000 rows will be used for training\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(\n",
    "                objective = 'multi:softmax', # 最简单：多分类softmax； 也可以使用，每个二分类\n",
    "                max_depth = 5, # 最大树深\n",
    "                n_estimators=300, # 树个数\n",
    "                learning_rate=0.01, # 学习率\n",
    "                nthread=4, # 线程个数\n",
    "                subsample=0.7, # 每次取样本比例，防止过拟合\n",
    "                colsample_bytree=0.7,\n",
    "                min_child_weight = 3,\n",
    "                silent=False) # bug信息输出\n",
    "\n",
    "\n",
    "if os.path.exists('rows_complete.txt'):\n",
    "    with open('rows_complete.txt', 'r') as f:\n",
    "        skipsize = int(f.readline())\n",
    "else:\n",
    "    skipsize = 0\n",
    "\n",
    "skip = 0 if skipsize==0 else range(1, skipsize)\n",
    "tchunksize = 1000000\n",
    "print('%d rows will be skipped and next %d rows will be used for training' % (skipsize, tchunksize))\n",
    "train = pd.read_csv('./output/train.csv', parse_dates=['date_time', 'srch_ci', 'srch_co'], skiprows=skip, nrows=tchunksize)\n",
    "train = train[train.is_booking==1]\n",
    "train = pd.merge(train, destinations, how='left', on='srch_destination_id')\n",
    "train = pd.merge(train, aggMod, how='left', on=['srch_destination_id','hotel_country','hotel_market'])\n",
    "pre_process(train)\n",
    "y = train.hotel_cluster\n",
    "train.drop(['cnt', 'hotel_cluster', 'is_booking'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:07] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-merror:0.79136\tvalidation_1-merror:0.81925\tvalidation_0-MAP@5:-0.18733\tvalidation_1-MAP@5:-0.16417\n",
      "Multiple eval metrics have been passed: 'validation_1-MAP@5' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-MAP@5 hasn't improved in 50 rounds.\n",
      "[1]\tvalidation_0-merror:0.76981\tvalidation_1-merror:0.79781\tvalidation_0-MAP@5:-0.20477\tvalidation_1-MAP@5:-0.18120\n",
      "[2]\tvalidation_0-merror:0.76486\tvalidation_1-merror:0.79379\tvalidation_0-MAP@5:-0.20867\tvalidation_1-MAP@5:-0.18385\n",
      "[3]\tvalidation_0-merror:0.76115\tvalidation_1-merror:0.79306\tvalidation_0-MAP@5:-0.21083\tvalidation_1-MAP@5:-0.18584\n",
      "[4]\tvalidation_0-merror:0.75917\tvalidation_1-merror:0.79117\tvalidation_0-MAP@5:-0.21236\tvalidation_1-MAP@5:-0.18668\n",
      "[5]\tvalidation_0-merror:0.75577\tvalidation_1-merror:0.78727\tvalidation_0-MAP@5:-0.21403\tvalidation_1-MAP@5:-0.18797\n",
      "[6]\tvalidation_0-merror:0.75333\tvalidation_1-merror:0.78599\tvalidation_0-MAP@5:-0.21528\tvalidation_1-MAP@5:-0.18916\n",
      "[7]\tvalidation_0-merror:0.75065\tvalidation_1-merror:0.78575\tvalidation_0-MAP@5:-0.21646\tvalidation_1-MAP@5:-0.18960\n",
      "[8]\tvalidation_0-merror:0.74893\tvalidation_1-merror:0.78429\tvalidation_0-MAP@5:-0.21733\tvalidation_1-MAP@5:-0.18996\n",
      "[9]\tvalidation_0-merror:0.74761\tvalidation_1-merror:0.78508\tvalidation_0-MAP@5:-0.21802\tvalidation_1-MAP@5:-0.18993\n",
      "[10]\tvalidation_0-merror:0.74679\tvalidation_1-merror:0.78301\tvalidation_0-MAP@5:-0.21866\tvalidation_1-MAP@5:-0.19106\n",
      "[11]\tvalidation_0-merror:0.74571\tvalidation_1-merror:0.78185\tvalidation_0-MAP@5:-0.21923\tvalidation_1-MAP@5:-0.19124\n",
      "[12]\tvalidation_0-merror:0.74444\tvalidation_1-merror:0.78149\tvalidation_0-MAP@5:-0.21969\tvalidation_1-MAP@5:-0.19118\n",
      "[13]\tvalidation_0-merror:0.74257\tvalidation_1-merror:0.78088\tvalidation_0-MAP@5:-0.22067\tvalidation_1-MAP@5:-0.19150\n",
      "[14]\tvalidation_0-merror:0.74144\tvalidation_1-merror:0.77929\tvalidation_0-MAP@5:-0.22114\tvalidation_1-MAP@5:-0.19175\n",
      "[15]\tvalidation_0-merror:0.74009\tvalidation_1-merror:0.77875\tvalidation_0-MAP@5:-0.22166\tvalidation_1-MAP@5:-0.19224\n",
      "[16]\tvalidation_0-merror:0.73927\tvalidation_1-merror:0.77972\tvalidation_0-MAP@5:-0.22213\tvalidation_1-MAP@5:-0.19219\n",
      "[17]\tvalidation_0-merror:0.73823\tvalidation_1-merror:0.77917\tvalidation_0-MAP@5:-0.22268\tvalidation_1-MAP@5:-0.19240\n",
      "[18]\tvalidation_0-merror:0.73762\tvalidation_1-merror:0.77966\tvalidation_0-MAP@5:-0.22297\tvalidation_1-MAP@5:-0.19255\n",
      "[19]\tvalidation_0-merror:0.73695\tvalidation_1-merror:0.78015\tvalidation_0-MAP@5:-0.22340\tvalidation_1-MAP@5:-0.19261\n",
      "[20]\tvalidation_0-merror:0.73554\tvalidation_1-merror:0.77881\tvalidation_0-MAP@5:-0.22379\tvalidation_1-MAP@5:-0.19307\n",
      "[21]\tvalidation_0-merror:0.73506\tvalidation_1-merror:0.77795\tvalidation_0-MAP@5:-0.22417\tvalidation_1-MAP@5:-0.19326\n",
      "[22]\tvalidation_0-merror:0.73452\tvalidation_1-merror:0.77850\tvalidation_0-MAP@5:-0.22461\tvalidation_1-MAP@5:-0.19317\n",
      "[23]\tvalidation_0-merror:0.73346\tvalidation_1-merror:0.77905\tvalidation_0-MAP@5:-0.22509\tvalidation_1-MAP@5:-0.19305\n",
      "[24]\tvalidation_0-merror:0.73269\tvalidation_1-merror:0.77893\tvalidation_0-MAP@5:-0.22538\tvalidation_1-MAP@5:-0.19302\n",
      "[25]\tvalidation_0-merror:0.73257\tvalidation_1-merror:0.77899\tvalidation_0-MAP@5:-0.22559\tvalidation_1-MAP@5:-0.19311\n",
      "[26]\tvalidation_0-merror:0.73144\tvalidation_1-merror:0.77862\tvalidation_0-MAP@5:-0.22582\tvalidation_1-MAP@5:-0.19342\n",
      "[27]\tvalidation_0-merror:0.73068\tvalidation_1-merror:0.77777\tvalidation_0-MAP@5:-0.22624\tvalidation_1-MAP@5:-0.19343\n",
      "[28]\tvalidation_0-merror:0.73007\tvalidation_1-merror:0.77844\tvalidation_0-MAP@5:-0.22672\tvalidation_1-MAP@5:-0.19348\n",
      "[29]\tvalidation_0-merror:0.72923\tvalidation_1-merror:0.77850\tvalidation_0-MAP@5:-0.22696\tvalidation_1-MAP@5:-0.19341\n",
      "[30]\tvalidation_0-merror:0.72884\tvalidation_1-merror:0.77832\tvalidation_0-MAP@5:-0.22725\tvalidation_1-MAP@5:-0.19341\n",
      "[31]\tvalidation_0-merror:0.72821\tvalidation_1-merror:0.77814\tvalidation_0-MAP@5:-0.22752\tvalidation_1-MAP@5:-0.19350\n",
      "[32]\tvalidation_0-merror:0.72780\tvalidation_1-merror:0.77814\tvalidation_0-MAP@5:-0.22779\tvalidation_1-MAP@5:-0.19369\n",
      "[33]\tvalidation_0-merror:0.72718\tvalidation_1-merror:0.77814\tvalidation_0-MAP@5:-0.22796\tvalidation_1-MAP@5:-0.19389\n",
      "[34]\tvalidation_0-merror:0.72687\tvalidation_1-merror:0.77728\tvalidation_0-MAP@5:-0.22818\tvalidation_1-MAP@5:-0.19387\n",
      "[35]\tvalidation_0-merror:0.72579\tvalidation_1-merror:0.77765\tvalidation_0-MAP@5:-0.22846\tvalidation_1-MAP@5:-0.19374\n",
      "[36]\tvalidation_0-merror:0.72561\tvalidation_1-merror:0.77698\tvalidation_0-MAP@5:-0.22869\tvalidation_1-MAP@5:-0.19392\n",
      "[37]\tvalidation_0-merror:0.72529\tvalidation_1-merror:0.77777\tvalidation_0-MAP@5:-0.22899\tvalidation_1-MAP@5:-0.19401\n",
      "[38]\tvalidation_0-merror:0.72510\tvalidation_1-merror:0.77802\tvalidation_0-MAP@5:-0.22922\tvalidation_1-MAP@5:-0.19383\n",
      "[39]\tvalidation_0-merror:0.72428\tvalidation_1-merror:0.77814\tvalidation_0-MAP@5:-0.22947\tvalidation_1-MAP@5:-0.19387\n",
      "[40]\tvalidation_0-merror:0.72361\tvalidation_1-merror:0.77722\tvalidation_0-MAP@5:-0.22978\tvalidation_1-MAP@5:-0.19413\n",
      "[41]\tvalidation_0-merror:0.72302\tvalidation_1-merror:0.77741\tvalidation_0-MAP@5:-0.22998\tvalidation_1-MAP@5:-0.19425\n",
      "[42]\tvalidation_0-merror:0.72244\tvalidation_1-merror:0.77680\tvalidation_0-MAP@5:-0.23031\tvalidation_1-MAP@5:-0.19427\n",
      "[43]\tvalidation_0-merror:0.72189\tvalidation_1-merror:0.77674\tvalidation_0-MAP@5:-0.23054\tvalidation_1-MAP@5:-0.19419\n",
      "[44]\tvalidation_0-merror:0.72151\tvalidation_1-merror:0.77722\tvalidation_0-MAP@5:-0.23078\tvalidation_1-MAP@5:-0.19426\n",
      "[45]\tvalidation_0-merror:0.72099\tvalidation_1-merror:0.77734\tvalidation_0-MAP@5:-0.23099\tvalidation_1-MAP@5:-0.19433\n",
      "[46]\tvalidation_0-merror:0.72072\tvalidation_1-merror:0.77753\tvalidation_0-MAP@5:-0.23116\tvalidation_1-MAP@5:-0.19432\n",
      "[47]\tvalidation_0-merror:0.71981\tvalidation_1-merror:0.77704\tvalidation_0-MAP@5:-0.23149\tvalidation_1-MAP@5:-0.19442\n",
      "[48]\tvalidation_0-merror:0.71909\tvalidation_1-merror:0.77771\tvalidation_0-MAP@5:-0.23180\tvalidation_1-MAP@5:-0.19440\n",
      "[49]\tvalidation_0-merror:0.71850\tvalidation_1-merror:0.77710\tvalidation_0-MAP@5:-0.23203\tvalidation_1-MAP@5:-0.19431\n",
      "[50]\tvalidation_0-merror:0.71818\tvalidation_1-merror:0.77649\tvalidation_0-MAP@5:-0.23212\tvalidation_1-MAP@5:-0.19439\n",
      "[51]\tvalidation_0-merror:0.71774\tvalidation_1-merror:0.77680\tvalidation_0-MAP@5:-0.23240\tvalidation_1-MAP@5:-0.19447\n",
      "[52]\tvalidation_0-merror:0.71749\tvalidation_1-merror:0.77668\tvalidation_0-MAP@5:-0.23256\tvalidation_1-MAP@5:-0.19444\n",
      "[53]\tvalidation_0-merror:0.71682\tvalidation_1-merror:0.77661\tvalidation_0-MAP@5:-0.23278\tvalidation_1-MAP@5:-0.19446\n",
      "[54]\tvalidation_0-merror:0.71634\tvalidation_1-merror:0.77600\tvalidation_0-MAP@5:-0.23305\tvalidation_1-MAP@5:-0.19447\n",
      "[55]\tvalidation_0-merror:0.71591\tvalidation_1-merror:0.77619\tvalidation_0-MAP@5:-0.23317\tvalidation_1-MAP@5:-0.19448\n",
      "[56]\tvalidation_0-merror:0.71568\tvalidation_1-merror:0.77607\tvalidation_0-MAP@5:-0.23332\tvalidation_1-MAP@5:-0.19455\n",
      "[57]\tvalidation_0-merror:0.71545\tvalidation_1-merror:0.77588\tvalidation_0-MAP@5:-0.23352\tvalidation_1-MAP@5:-0.19461\n",
      "[58]\tvalidation_0-merror:0.71501\tvalidation_1-merror:0.77576\tvalidation_0-MAP@5:-0.23370\tvalidation_1-MAP@5:-0.19470\n",
      "[59]\tvalidation_0-merror:0.71457\tvalidation_1-merror:0.77570\tvalidation_0-MAP@5:-0.23386\tvalidation_1-MAP@5:-0.19468\n",
      "[60]\tvalidation_0-merror:0.71407\tvalidation_1-merror:0.77613\tvalidation_0-MAP@5:-0.23400\tvalidation_1-MAP@5:-0.19464\n",
      "[61]\tvalidation_0-merror:0.71385\tvalidation_1-merror:0.77588\tvalidation_0-MAP@5:-0.23423\tvalidation_1-MAP@5:-0.19475\n",
      "[62]\tvalidation_0-merror:0.71347\tvalidation_1-merror:0.77607\tvalidation_0-MAP@5:-0.23444\tvalidation_1-MAP@5:-0.19461\n",
      "[63]\tvalidation_0-merror:0.71276\tvalidation_1-merror:0.77625\tvalidation_0-MAP@5:-0.23464\tvalidation_1-MAP@5:-0.19472\n",
      "[64]\tvalidation_0-merror:0.71216\tvalidation_1-merror:0.77619\tvalidation_0-MAP@5:-0.23488\tvalidation_1-MAP@5:-0.19473\n",
      "[65]\tvalidation_0-merror:0.71209\tvalidation_1-merror:0.77582\tvalidation_0-MAP@5:-0.23506\tvalidation_1-MAP@5:-0.19483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66]\tvalidation_0-merror:0.71184\tvalidation_1-merror:0.77558\tvalidation_0-MAP@5:-0.23522\tvalidation_1-MAP@5:-0.19480\n",
      "[67]\tvalidation_0-merror:0.71152\tvalidation_1-merror:0.77515\tvalidation_0-MAP@5:-0.23539\tvalidation_1-MAP@5:-0.19494\n",
      "[68]\tvalidation_0-merror:0.71157\tvalidation_1-merror:0.77576\tvalidation_0-MAP@5:-0.23555\tvalidation_1-MAP@5:-0.19487\n",
      "[69]\tvalidation_0-merror:0.71084\tvalidation_1-merror:0.77558\tvalidation_0-MAP@5:-0.23578\tvalidation_1-MAP@5:-0.19488\n",
      "[70]\tvalidation_0-merror:0.71056\tvalidation_1-merror:0.77515\tvalidation_0-MAP@5:-0.23594\tvalidation_1-MAP@5:-0.19492\n",
      "[71]\tvalidation_0-merror:0.70998\tvalidation_1-merror:0.77576\tvalidation_0-MAP@5:-0.23611\tvalidation_1-MAP@5:-0.19488\n",
      "[72]\tvalidation_0-merror:0.70942\tvalidation_1-merror:0.77564\tvalidation_0-MAP@5:-0.23630\tvalidation_1-MAP@5:-0.19482\n",
      "[73]\tvalidation_0-merror:0.70930\tvalidation_1-merror:0.77552\tvalidation_0-MAP@5:-0.23647\tvalidation_1-MAP@5:-0.19485\n",
      "[74]\tvalidation_0-merror:0.70903\tvalidation_1-merror:0.77497\tvalidation_0-MAP@5:-0.23669\tvalidation_1-MAP@5:-0.19488\n",
      "[75]\tvalidation_0-merror:0.70880\tvalidation_1-merror:0.77503\tvalidation_0-MAP@5:-0.23675\tvalidation_1-MAP@5:-0.19491\n",
      "[76]\tvalidation_0-merror:0.70827\tvalidation_1-merror:0.77473\tvalidation_0-MAP@5:-0.23704\tvalidation_1-MAP@5:-0.19506\n",
      "[77]\tvalidation_0-merror:0.70836\tvalidation_1-merror:0.77491\tvalidation_0-MAP@5:-0.23718\tvalidation_1-MAP@5:-0.19498\n",
      "[78]\tvalidation_0-merror:0.70762\tvalidation_1-merror:0.77467\tvalidation_0-MAP@5:-0.23734\tvalidation_1-MAP@5:-0.19509\n",
      "[79]\tvalidation_0-merror:0.70758\tvalidation_1-merror:0.77442\tvalidation_0-MAP@5:-0.23747\tvalidation_1-MAP@5:-0.19509\n",
      "[80]\tvalidation_0-merror:0.70703\tvalidation_1-merror:0.77430\tvalidation_0-MAP@5:-0.23771\tvalidation_1-MAP@5:-0.19502\n",
      "[81]\tvalidation_0-merror:0.70679\tvalidation_1-merror:0.77485\tvalidation_0-MAP@5:-0.23783\tvalidation_1-MAP@5:-0.19510\n",
      "[82]\tvalidation_0-merror:0.70648\tvalidation_1-merror:0.77460\tvalidation_0-MAP@5:-0.23800\tvalidation_1-MAP@5:-0.19502\n",
      "[83]\tvalidation_0-merror:0.70606\tvalidation_1-merror:0.77430\tvalidation_0-MAP@5:-0.23824\tvalidation_1-MAP@5:-0.19509\n",
      "[84]\tvalidation_0-merror:0.70559\tvalidation_1-merror:0.77424\tvalidation_0-MAP@5:-0.23839\tvalidation_1-MAP@5:-0.19498\n",
      "[85]\tvalidation_0-merror:0.70533\tvalidation_1-merror:0.77412\tvalidation_0-MAP@5:-0.23854\tvalidation_1-MAP@5:-0.19494\n",
      "[86]\tvalidation_0-merror:0.70536\tvalidation_1-merror:0.77406\tvalidation_0-MAP@5:-0.23867\tvalidation_1-MAP@5:-0.19505\n",
      "[87]\tvalidation_0-merror:0.70507\tvalidation_1-merror:0.77393\tvalidation_0-MAP@5:-0.23883\tvalidation_1-MAP@5:-0.19521\n",
      "[88]\tvalidation_0-merror:0.70452\tvalidation_1-merror:0.77393\tvalidation_0-MAP@5:-0.23905\tvalidation_1-MAP@5:-0.19502\n",
      "[89]\tvalidation_0-merror:0.70415\tvalidation_1-merror:0.77418\tvalidation_0-MAP@5:-0.23930\tvalidation_1-MAP@5:-0.19496\n",
      "[90]\tvalidation_0-merror:0.70380\tvalidation_1-merror:0.77412\tvalidation_0-MAP@5:-0.23944\tvalidation_1-MAP@5:-0.19497\n",
      "[91]\tvalidation_0-merror:0.70348\tvalidation_1-merror:0.77460\tvalidation_0-MAP@5:-0.23958\tvalidation_1-MAP@5:-0.19494\n",
      "[92]\tvalidation_0-merror:0.70332\tvalidation_1-merror:0.77467\tvalidation_0-MAP@5:-0.23971\tvalidation_1-MAP@5:-0.19499\n",
      "[93]\tvalidation_0-merror:0.70301\tvalidation_1-merror:0.77473\tvalidation_0-MAP@5:-0.23982\tvalidation_1-MAP@5:-0.19504\n",
      "[94]\tvalidation_0-merror:0.70272\tvalidation_1-merror:0.77454\tvalidation_0-MAP@5:-0.23994\tvalidation_1-MAP@5:-0.19510\n",
      "[95]\tvalidation_0-merror:0.70242\tvalidation_1-merror:0.77479\tvalidation_0-MAP@5:-0.24010\tvalidation_1-MAP@5:-0.19502\n",
      "[96]\tvalidation_0-merror:0.70213\tvalidation_1-merror:0.77442\tvalidation_0-MAP@5:-0.24023\tvalidation_1-MAP@5:-0.19507\n",
      "[97]\tvalidation_0-merror:0.70191\tvalidation_1-merror:0.77442\tvalidation_0-MAP@5:-0.24037\tvalidation_1-MAP@5:-0.19507\n",
      "[98]\tvalidation_0-merror:0.70164\tvalidation_1-merror:0.77424\tvalidation_0-MAP@5:-0.24051\tvalidation_1-MAP@5:-0.19526\n",
      "[99]\tvalidation_0-merror:0.70138\tvalidation_1-merror:0.77399\tvalidation_0-MAP@5:-0.24066\tvalidation_1-MAP@5:-0.19523\n",
      "[100]\tvalidation_0-merror:0.70126\tvalidation_1-merror:0.77436\tvalidation_0-MAP@5:-0.24088\tvalidation_1-MAP@5:-0.19526\n",
      "[101]\tvalidation_0-merror:0.70111\tvalidation_1-merror:0.77369\tvalidation_0-MAP@5:-0.24100\tvalidation_1-MAP@5:-0.19535\n",
      "[102]\tvalidation_0-merror:0.70070\tvalidation_1-merror:0.77387\tvalidation_0-MAP@5:-0.24111\tvalidation_1-MAP@5:-0.19538\n",
      "[103]\tvalidation_0-merror:0.70047\tvalidation_1-merror:0.77351\tvalidation_0-MAP@5:-0.24121\tvalidation_1-MAP@5:-0.19546\n",
      "[104]\tvalidation_0-merror:0.70038\tvalidation_1-merror:0.77333\tvalidation_0-MAP@5:-0.24138\tvalidation_1-MAP@5:-0.19544\n",
      "[105]\tvalidation_0-merror:0.70021\tvalidation_1-merror:0.77399\tvalidation_0-MAP@5:-0.24152\tvalidation_1-MAP@5:-0.19532\n",
      "[106]\tvalidation_0-merror:0.69988\tvalidation_1-merror:0.77399\tvalidation_0-MAP@5:-0.24168\tvalidation_1-MAP@5:-0.19534\n",
      "[107]\tvalidation_0-merror:0.69978\tvalidation_1-merror:0.77369\tvalidation_0-MAP@5:-0.24179\tvalidation_1-MAP@5:-0.19522\n",
      "[108]\tvalidation_0-merror:0.69925\tvalidation_1-merror:0.77357\tvalidation_0-MAP@5:-0.24197\tvalidation_1-MAP@5:-0.19530\n",
      "[109]\tvalidation_0-merror:0.69907\tvalidation_1-merror:0.77314\tvalidation_0-MAP@5:-0.24212\tvalidation_1-MAP@5:-0.19541\n",
      "[110]\tvalidation_0-merror:0.69870\tvalidation_1-merror:0.77345\tvalidation_0-MAP@5:-0.24224\tvalidation_1-MAP@5:-0.19537\n",
      "[111]\tvalidation_0-merror:0.69831\tvalidation_1-merror:0.77333\tvalidation_0-MAP@5:-0.24237\tvalidation_1-MAP@5:-0.19532\n",
      "[112]\tvalidation_0-merror:0.69815\tvalidation_1-merror:0.77320\tvalidation_0-MAP@5:-0.24257\tvalidation_1-MAP@5:-0.19531\n",
      "[113]\tvalidation_0-merror:0.69794\tvalidation_1-merror:0.77339\tvalidation_0-MAP@5:-0.24269\tvalidation_1-MAP@5:-0.19530\n",
      "[114]\tvalidation_0-merror:0.69764\tvalidation_1-merror:0.77357\tvalidation_0-MAP@5:-0.24282\tvalidation_1-MAP@5:-0.19527\n",
      "[115]\tvalidation_0-merror:0.69750\tvalidation_1-merror:0.77387\tvalidation_0-MAP@5:-0.24289\tvalidation_1-MAP@5:-0.19526\n",
      "[116]\tvalidation_0-merror:0.69730\tvalidation_1-merror:0.77369\tvalidation_0-MAP@5:-0.24300\tvalidation_1-MAP@5:-0.19524\n",
      "[117]\tvalidation_0-merror:0.69694\tvalidation_1-merror:0.77381\tvalidation_0-MAP@5:-0.24314\tvalidation_1-MAP@5:-0.19521\n",
      "[118]\tvalidation_0-merror:0.69651\tvalidation_1-merror:0.77363\tvalidation_0-MAP@5:-0.24337\tvalidation_1-MAP@5:-0.19540\n",
      "[119]\tvalidation_0-merror:0.69633\tvalidation_1-merror:0.77363\tvalidation_0-MAP@5:-0.24353\tvalidation_1-MAP@5:-0.19539\n",
      "[120]\tvalidation_0-merror:0.69622\tvalidation_1-merror:0.77333\tvalidation_0-MAP@5:-0.24362\tvalidation_1-MAP@5:-0.19547\n",
      "[121]\tvalidation_0-merror:0.69592\tvalidation_1-merror:0.77345\tvalidation_0-MAP@5:-0.24380\tvalidation_1-MAP@5:-0.19548\n",
      "[122]\tvalidation_0-merror:0.69555\tvalidation_1-merror:0.77375\tvalidation_0-MAP@5:-0.24392\tvalidation_1-MAP@5:-0.19536\n",
      "[123]\tvalidation_0-merror:0.69531\tvalidation_1-merror:0.77351\tvalidation_0-MAP@5:-0.24407\tvalidation_1-MAP@5:-0.19540\n",
      "[124]\tvalidation_0-merror:0.69509\tvalidation_1-merror:0.77381\tvalidation_0-MAP@5:-0.24416\tvalidation_1-MAP@5:-0.19539\n",
      "[125]\tvalidation_0-merror:0.69485\tvalidation_1-merror:0.77345\tvalidation_0-MAP@5:-0.24423\tvalidation_1-MAP@5:-0.19539\n",
      "[126]\tvalidation_0-merror:0.69453\tvalidation_1-merror:0.77333\tvalidation_0-MAP@5:-0.24435\tvalidation_1-MAP@5:-0.19533\n",
      "[127]\tvalidation_0-merror:0.69420\tvalidation_1-merror:0.77333\tvalidation_0-MAP@5:-0.24451\tvalidation_1-MAP@5:-0.19535\n",
      "[128]\tvalidation_0-merror:0.69400\tvalidation_1-merror:0.77339\tvalidation_0-MAP@5:-0.24458\tvalidation_1-MAP@5:-0.19531\n",
      "[129]\tvalidation_0-merror:0.69369\tvalidation_1-merror:0.77357\tvalidation_0-MAP@5:-0.24472\tvalidation_1-MAP@5:-0.19527\n",
      "[130]\tvalidation_0-merror:0.69342\tvalidation_1-merror:0.77393\tvalidation_0-MAP@5:-0.24482\tvalidation_1-MAP@5:-0.19506\n",
      "[131]\tvalidation_0-merror:0.69321\tvalidation_1-merror:0.77375\tvalidation_0-MAP@5:-0.24500\tvalidation_1-MAP@5:-0.19509\n",
      "[132]\tvalidation_0-merror:0.69284\tvalidation_1-merror:0.77375\tvalidation_0-MAP@5:-0.24508\tvalidation_1-MAP@5:-0.19504\n",
      "[133]\tvalidation_0-merror:0.69281\tvalidation_1-merror:0.77375\tvalidation_0-MAP@5:-0.24521\tvalidation_1-MAP@5:-0.19508\n",
      "[134]\tvalidation_0-merror:0.69238\tvalidation_1-merror:0.77399\tvalidation_0-MAP@5:-0.24538\tvalidation_1-MAP@5:-0.19514\n",
      "[135]\tvalidation_0-merror:0.69202\tvalidation_1-merror:0.77406\tvalidation_0-MAP@5:-0.24552\tvalidation_1-MAP@5:-0.19501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[136]\tvalidation_0-merror:0.69165\tvalidation_1-merror:0.77363\tvalidation_0-MAP@5:-0.24565\tvalidation_1-MAP@5:-0.19520\n",
      "[137]\tvalidation_0-merror:0.69129\tvalidation_1-merror:0.77387\tvalidation_0-MAP@5:-0.24579\tvalidation_1-MAP@5:-0.19517\n",
      "[138]\tvalidation_0-merror:0.69117\tvalidation_1-merror:0.77369\tvalidation_0-MAP@5:-0.24595\tvalidation_1-MAP@5:-0.19529\n",
      "[139]\tvalidation_0-merror:0.69085\tvalidation_1-merror:0.77369\tvalidation_0-MAP@5:-0.24604\tvalidation_1-MAP@5:-0.19523\n",
      "[140]\tvalidation_0-merror:0.69076\tvalidation_1-merror:0.77399\tvalidation_0-MAP@5:-0.24612\tvalidation_1-MAP@5:-0.19522\n",
      "[141]\tvalidation_0-merror:0.69060\tvalidation_1-merror:0.77375\tvalidation_0-MAP@5:-0.24627\tvalidation_1-MAP@5:-0.19529\n",
      "[142]\tvalidation_0-merror:0.69040\tvalidation_1-merror:0.77454\tvalidation_0-MAP@5:-0.24636\tvalidation_1-MAP@5:-0.19519\n",
      "[143]\tvalidation_0-merror:0.68992\tvalidation_1-merror:0.77442\tvalidation_0-MAP@5:-0.24653\tvalidation_1-MAP@5:-0.19520\n",
      "[144]\tvalidation_0-merror:0.68966\tvalidation_1-merror:0.77473\tvalidation_0-MAP@5:-0.24665\tvalidation_1-MAP@5:-0.19517\n",
      "[145]\tvalidation_0-merror:0.68916\tvalidation_1-merror:0.77399\tvalidation_0-MAP@5:-0.24682\tvalidation_1-MAP@5:-0.19537\n",
      "[146]\tvalidation_0-merror:0.68893\tvalidation_1-merror:0.77387\tvalidation_0-MAP@5:-0.24696\tvalidation_1-MAP@5:-0.19542\n",
      "[147]\tvalidation_0-merror:0.68908\tvalidation_1-merror:0.77393\tvalidation_0-MAP@5:-0.24705\tvalidation_1-MAP@5:-0.19538\n",
      "[148]\tvalidation_0-merror:0.68855\tvalidation_1-merror:0.77363\tvalidation_0-MAP@5:-0.24722\tvalidation_1-MAP@5:-0.19547\n",
      "[149]\tvalidation_0-merror:0.68850\tvalidation_1-merror:0.77375\tvalidation_0-MAP@5:-0.24728\tvalidation_1-MAP@5:-0.19538\n",
      "[150]\tvalidation_0-merror:0.68827\tvalidation_1-merror:0.77375\tvalidation_0-MAP@5:-0.24738\tvalidation_1-MAP@5:-0.19549\n",
      "[151]\tvalidation_0-merror:0.68788\tvalidation_1-merror:0.77393\tvalidation_0-MAP@5:-0.24759\tvalidation_1-MAP@5:-0.19537\n",
      "[152]\tvalidation_0-merror:0.68753\tvalidation_1-merror:0.77418\tvalidation_0-MAP@5:-0.24773\tvalidation_1-MAP@5:-0.19541\n",
      "[153]\tvalidation_0-merror:0.68757\tvalidation_1-merror:0.77412\tvalidation_0-MAP@5:-0.24777\tvalidation_1-MAP@5:-0.19549\n",
      "[154]\tvalidation_0-merror:0.68725\tvalidation_1-merror:0.77430\tvalidation_0-MAP@5:-0.24792\tvalidation_1-MAP@5:-0.19544\n",
      "[155]\tvalidation_0-merror:0.68692\tvalidation_1-merror:0.77393\tvalidation_0-MAP@5:-0.24811\tvalidation_1-MAP@5:-0.19551\n",
      "[156]\tvalidation_0-merror:0.68693\tvalidation_1-merror:0.77387\tvalidation_0-MAP@5:-0.24812\tvalidation_1-MAP@5:-0.19551\n",
      "[157]\tvalidation_0-merror:0.68672\tvalidation_1-merror:0.77363\tvalidation_0-MAP@5:-0.24824\tvalidation_1-MAP@5:-0.19548\n",
      "[158]\tvalidation_0-merror:0.68652\tvalidation_1-merror:0.77381\tvalidation_0-MAP@5:-0.24835\tvalidation_1-MAP@5:-0.19541\n",
      "[159]\tvalidation_0-merror:0.68635\tvalidation_1-merror:0.77387\tvalidation_0-MAP@5:-0.24839\tvalidation_1-MAP@5:-0.19530\n",
      "[160]\tvalidation_0-merror:0.68613\tvalidation_1-merror:0.77351\tvalidation_0-MAP@5:-0.24856\tvalidation_1-MAP@5:-0.19541\n",
      "[161]\tvalidation_0-merror:0.68561\tvalidation_1-merror:0.77351\tvalidation_0-MAP@5:-0.24873\tvalidation_1-MAP@5:-0.19542\n",
      "[162]\tvalidation_0-merror:0.68524\tvalidation_1-merror:0.77351\tvalidation_0-MAP@5:-0.24881\tvalidation_1-MAP@5:-0.19541\n",
      "[163]\tvalidation_0-merror:0.68495\tvalidation_1-merror:0.77308\tvalidation_0-MAP@5:-0.24899\tvalidation_1-MAP@5:-0.19558\n",
      "[164]\tvalidation_0-merror:0.68479\tvalidation_1-merror:0.77333\tvalidation_0-MAP@5:-0.24905\tvalidation_1-MAP@5:-0.19552\n",
      "[165]\tvalidation_0-merror:0.68459\tvalidation_1-merror:0.77363\tvalidation_0-MAP@5:-0.24912\tvalidation_1-MAP@5:-0.19555\n",
      "[166]\tvalidation_0-merror:0.68451\tvalidation_1-merror:0.77345\tvalidation_0-MAP@5:-0.24926\tvalidation_1-MAP@5:-0.19550\n",
      "[167]\tvalidation_0-merror:0.68422\tvalidation_1-merror:0.77357\tvalidation_0-MAP@5:-0.24939\tvalidation_1-MAP@5:-0.19551\n",
      "[168]\tvalidation_0-merror:0.68375\tvalidation_1-merror:0.77351\tvalidation_0-MAP@5:-0.24953\tvalidation_1-MAP@5:-0.19551\n",
      "[169]\tvalidation_0-merror:0.68367\tvalidation_1-merror:0.77363\tvalidation_0-MAP@5:-0.24961\tvalidation_1-MAP@5:-0.19546\n",
      "[170]\tvalidation_0-merror:0.68358\tvalidation_1-merror:0.77351\tvalidation_0-MAP@5:-0.24973\tvalidation_1-MAP@5:-0.19553\n",
      "[171]\tvalidation_0-merror:0.68319\tvalidation_1-merror:0.77363\tvalidation_0-MAP@5:-0.24985\tvalidation_1-MAP@5:-0.19543\n",
      "[172]\tvalidation_0-merror:0.68285\tvalidation_1-merror:0.77333\tvalidation_0-MAP@5:-0.25001\tvalidation_1-MAP@5:-0.19547\n",
      "[173]\tvalidation_0-merror:0.68273\tvalidation_1-merror:0.77375\tvalidation_0-MAP@5:-0.25005\tvalidation_1-MAP@5:-0.19543\n",
      "[174]\tvalidation_0-merror:0.68249\tvalidation_1-merror:0.77387\tvalidation_0-MAP@5:-0.25015\tvalidation_1-MAP@5:-0.19539\n",
      "[175]\tvalidation_0-merror:0.68241\tvalidation_1-merror:0.77412\tvalidation_0-MAP@5:-0.25025\tvalidation_1-MAP@5:-0.19531\n",
      "[176]\tvalidation_0-merror:0.68205\tvalidation_1-merror:0.77442\tvalidation_0-MAP@5:-0.25045\tvalidation_1-MAP@5:-0.19520\n",
      "[177]\tvalidation_0-merror:0.68188\tvalidation_1-merror:0.77430\tvalidation_0-MAP@5:-0.25054\tvalidation_1-MAP@5:-0.19532\n",
      "[178]\tvalidation_0-merror:0.68137\tvalidation_1-merror:0.77387\tvalidation_0-MAP@5:-0.25067\tvalidation_1-MAP@5:-0.19539\n",
      "[179]\tvalidation_0-merror:0.68115\tvalidation_1-merror:0.77399\tvalidation_0-MAP@5:-0.25079\tvalidation_1-MAP@5:-0.19523\n",
      "[180]\tvalidation_0-merror:0.68074\tvalidation_1-merror:0.77399\tvalidation_0-MAP@5:-0.25089\tvalidation_1-MAP@5:-0.19530\n",
      "[181]\tvalidation_0-merror:0.68075\tvalidation_1-merror:0.77430\tvalidation_0-MAP@5:-0.25097\tvalidation_1-MAP@5:-0.19526\n",
      "[182]\tvalidation_0-merror:0.68039\tvalidation_1-merror:0.77412\tvalidation_0-MAP@5:-0.25114\tvalidation_1-MAP@5:-0.19536\n",
      "[183]\tvalidation_0-merror:0.68022\tvalidation_1-merror:0.77418\tvalidation_0-MAP@5:-0.25126\tvalidation_1-MAP@5:-0.19532\n",
      "[184]\tvalidation_0-merror:0.67990\tvalidation_1-merror:0.77375\tvalidation_0-MAP@5:-0.25138\tvalidation_1-MAP@5:-0.19532\n",
      "[185]\tvalidation_0-merror:0.67968\tvalidation_1-merror:0.77369\tvalidation_0-MAP@5:-0.25149\tvalidation_1-MAP@5:-0.19542\n",
      "[186]\tvalidation_0-merror:0.67940\tvalidation_1-merror:0.77375\tvalidation_0-MAP@5:-0.25162\tvalidation_1-MAP@5:-0.19537\n",
      "[187]\tvalidation_0-merror:0.67911\tvalidation_1-merror:0.77375\tvalidation_0-MAP@5:-0.25176\tvalidation_1-MAP@5:-0.19541\n",
      "[188]\tvalidation_0-merror:0.67880\tvalidation_1-merror:0.77326\tvalidation_0-MAP@5:-0.25184\tvalidation_1-MAP@5:-0.19548\n",
      "[189]\tvalidation_0-merror:0.67871\tvalidation_1-merror:0.77308\tvalidation_0-MAP@5:-0.25199\tvalidation_1-MAP@5:-0.19554\n",
      "[190]\tvalidation_0-merror:0.67841\tvalidation_1-merror:0.77302\tvalidation_0-MAP@5:-0.25212\tvalidation_1-MAP@5:-0.19548\n",
      "[191]\tvalidation_0-merror:0.67822\tvalidation_1-merror:0.77296\tvalidation_0-MAP@5:-0.25221\tvalidation_1-MAP@5:-0.19545\n",
      "[192]\tvalidation_0-merror:0.67806\tvalidation_1-merror:0.77296\tvalidation_0-MAP@5:-0.25225\tvalidation_1-MAP@5:-0.19549\n",
      "[193]\tvalidation_0-merror:0.67781\tvalidation_1-merror:0.77308\tvalidation_0-MAP@5:-0.25237\tvalidation_1-MAP@5:-0.19554\n",
      "[194]\tvalidation_0-merror:0.67758\tvalidation_1-merror:0.77314\tvalidation_0-MAP@5:-0.25245\tvalidation_1-MAP@5:-0.19555\n",
      "[195]\tvalidation_0-merror:0.67739\tvalidation_1-merror:0.77308\tvalidation_0-MAP@5:-0.25255\tvalidation_1-MAP@5:-0.19554\n",
      "[196]\tvalidation_0-merror:0.67723\tvalidation_1-merror:0.77302\tvalidation_0-MAP@5:-0.25261\tvalidation_1-MAP@5:-0.19558\n",
      "[197]\tvalidation_0-merror:0.67684\tvalidation_1-merror:0.77308\tvalidation_0-MAP@5:-0.25272\tvalidation_1-MAP@5:-0.19554\n",
      "[198]\tvalidation_0-merror:0.67666\tvalidation_1-merror:0.77302\tvalidation_0-MAP@5:-0.25284\tvalidation_1-MAP@5:-0.19568\n",
      "[199]\tvalidation_0-merror:0.67653\tvalidation_1-merror:0.77302\tvalidation_0-MAP@5:-0.25297\tvalidation_1-MAP@5:-0.19577\n",
      "[200]\tvalidation_0-merror:0.67623\tvalidation_1-merror:0.77302\tvalidation_0-MAP@5:-0.25305\tvalidation_1-MAP@5:-0.19569\n",
      "[201]\tvalidation_0-merror:0.67603\tvalidation_1-merror:0.77278\tvalidation_0-MAP@5:-0.25319\tvalidation_1-MAP@5:-0.19571\n",
      "[202]\tvalidation_0-merror:0.67580\tvalidation_1-merror:0.77308\tvalidation_0-MAP@5:-0.25326\tvalidation_1-MAP@5:-0.19562\n",
      "[203]\tvalidation_0-merror:0.67547\tvalidation_1-merror:0.77308\tvalidation_0-MAP@5:-0.25339\tvalidation_1-MAP@5:-0.19567\n",
      "[204]\tvalidation_0-merror:0.67522\tvalidation_1-merror:0.77314\tvalidation_0-MAP@5:-0.25352\tvalidation_1-MAP@5:-0.19575\n",
      "[205]\tvalidation_0-merror:0.67509\tvalidation_1-merror:0.77333\tvalidation_0-MAP@5:-0.25363\tvalidation_1-MAP@5:-0.19570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[206]\tvalidation_0-merror:0.67478\tvalidation_1-merror:0.77357\tvalidation_0-MAP@5:-0.25376\tvalidation_1-MAP@5:-0.19575\n",
      "[207]\tvalidation_0-merror:0.67443\tvalidation_1-merror:0.77326\tvalidation_0-MAP@5:-0.25389\tvalidation_1-MAP@5:-0.19570\n",
      "[208]\tvalidation_0-merror:0.67434\tvalidation_1-merror:0.77351\tvalidation_0-MAP@5:-0.25399\tvalidation_1-MAP@5:-0.19566\n",
      "[209]\tvalidation_0-merror:0.67408\tvalidation_1-merror:0.77339\tvalidation_0-MAP@5:-0.25413\tvalidation_1-MAP@5:-0.19572\n",
      "[210]\tvalidation_0-merror:0.67384\tvalidation_1-merror:0.77375\tvalidation_0-MAP@5:-0.25424\tvalidation_1-MAP@5:-0.19566\n",
      "[211]\tvalidation_0-merror:0.67353\tvalidation_1-merror:0.77357\tvalidation_0-MAP@5:-0.25440\tvalidation_1-MAP@5:-0.19568\n",
      "[212]\tvalidation_0-merror:0.67326\tvalidation_1-merror:0.77363\tvalidation_0-MAP@5:-0.25455\tvalidation_1-MAP@5:-0.19567\n",
      "[213]\tvalidation_0-merror:0.67306\tvalidation_1-merror:0.77381\tvalidation_0-MAP@5:-0.25464\tvalidation_1-MAP@5:-0.19565\n",
      "[214]\tvalidation_0-merror:0.67289\tvalidation_1-merror:0.77406\tvalidation_0-MAP@5:-0.25472\tvalidation_1-MAP@5:-0.19554\n",
      "[215]\tvalidation_0-merror:0.67279\tvalidation_1-merror:0.77399\tvalidation_0-MAP@5:-0.25481\tvalidation_1-MAP@5:-0.19555\n",
      "[216]\tvalidation_0-merror:0.67265\tvalidation_1-merror:0.77393\tvalidation_0-MAP@5:-0.25493\tvalidation_1-MAP@5:-0.19561\n",
      "[217]\tvalidation_0-merror:0.67236\tvalidation_1-merror:0.77399\tvalidation_0-MAP@5:-0.25505\tvalidation_1-MAP@5:-0.19561\n",
      "[218]\tvalidation_0-merror:0.67229\tvalidation_1-merror:0.77393\tvalidation_0-MAP@5:-0.25511\tvalidation_1-MAP@5:-0.19571\n",
      "[219]\tvalidation_0-merror:0.67186\tvalidation_1-merror:0.77424\tvalidation_0-MAP@5:-0.25527\tvalidation_1-MAP@5:-0.19565\n",
      "[220]\tvalidation_0-merror:0.67165\tvalidation_1-merror:0.77399\tvalidation_0-MAP@5:-0.25537\tvalidation_1-MAP@5:-0.19569\n",
      "[221]\tvalidation_0-merror:0.67133\tvalidation_1-merror:0.77387\tvalidation_0-MAP@5:-0.25552\tvalidation_1-MAP@5:-0.19570\n",
      "[222]\tvalidation_0-merror:0.67134\tvalidation_1-merror:0.77381\tvalidation_0-MAP@5:-0.25555\tvalidation_1-MAP@5:-0.19566\n",
      "[223]\tvalidation_0-merror:0.67102\tvalidation_1-merror:0.77387\tvalidation_0-MAP@5:-0.25564\tvalidation_1-MAP@5:-0.19570\n",
      "[224]\tvalidation_0-merror:0.67060\tvalidation_1-merror:0.77399\tvalidation_0-MAP@5:-0.25577\tvalidation_1-MAP@5:-0.19574\n",
      "[225]\tvalidation_0-merror:0.67038\tvalidation_1-merror:0.77442\tvalidation_0-MAP@5:-0.25584\tvalidation_1-MAP@5:-0.19565\n",
      "[226]\tvalidation_0-merror:0.67017\tvalidation_1-merror:0.77430\tvalidation_0-MAP@5:-0.25596\tvalidation_1-MAP@5:-0.19563\n",
      "[227]\tvalidation_0-merror:0.66977\tvalidation_1-merror:0.77412\tvalidation_0-MAP@5:-0.25611\tvalidation_1-MAP@5:-0.19557\n",
      "[228]\tvalidation_0-merror:0.66967\tvalidation_1-merror:0.77430\tvalidation_0-MAP@5:-0.25620\tvalidation_1-MAP@5:-0.19561\n",
      "[229]\tvalidation_0-merror:0.66924\tvalidation_1-merror:0.77418\tvalidation_0-MAP@5:-0.25637\tvalidation_1-MAP@5:-0.19565\n",
      "[230]\tvalidation_0-merror:0.66913\tvalidation_1-merror:0.77424\tvalidation_0-MAP@5:-0.25644\tvalidation_1-MAP@5:-0.19560\n",
      "[231]\tvalidation_0-merror:0.66901\tvalidation_1-merror:0.77424\tvalidation_0-MAP@5:-0.25652\tvalidation_1-MAP@5:-0.19562\n",
      "[232]\tvalidation_0-merror:0.66893\tvalidation_1-merror:0.77412\tvalidation_0-MAP@5:-0.25658\tvalidation_1-MAP@5:-0.19570\n",
      "[233]\tvalidation_0-merror:0.66874\tvalidation_1-merror:0.77393\tvalidation_0-MAP@5:-0.25672\tvalidation_1-MAP@5:-0.19579\n",
      "[234]\tvalidation_0-merror:0.66863\tvalidation_1-merror:0.77375\tvalidation_0-MAP@5:-0.25682\tvalidation_1-MAP@5:-0.19577\n",
      "[235]\tvalidation_0-merror:0.66828\tvalidation_1-merror:0.77399\tvalidation_0-MAP@5:-0.25694\tvalidation_1-MAP@5:-0.19572\n",
      "[236]\tvalidation_0-merror:0.66813\tvalidation_1-merror:0.77387\tvalidation_0-MAP@5:-0.25704\tvalidation_1-MAP@5:-0.19577\n",
      "[237]\tvalidation_0-merror:0.66802\tvalidation_1-merror:0.77399\tvalidation_0-MAP@5:-0.25708\tvalidation_1-MAP@5:-0.19581\n",
      "[238]\tvalidation_0-merror:0.66752\tvalidation_1-merror:0.77399\tvalidation_0-MAP@5:-0.25725\tvalidation_1-MAP@5:-0.19577\n",
      "[239]\tvalidation_0-merror:0.66722\tvalidation_1-merror:0.77357\tvalidation_0-MAP@5:-0.25737\tvalidation_1-MAP@5:-0.19579\n",
      "[240]\tvalidation_0-merror:0.66708\tvalidation_1-merror:0.77399\tvalidation_0-MAP@5:-0.25744\tvalidation_1-MAP@5:-0.19573\n",
      "[241]\tvalidation_0-merror:0.66673\tvalidation_1-merror:0.77357\tvalidation_0-MAP@5:-0.25755\tvalidation_1-MAP@5:-0.19578\n",
      "[242]\tvalidation_0-merror:0.66650\tvalidation_1-merror:0.77345\tvalidation_0-MAP@5:-0.25763\tvalidation_1-MAP@5:-0.19578\n",
      "[243]\tvalidation_0-merror:0.66613\tvalidation_1-merror:0.77375\tvalidation_0-MAP@5:-0.25775\tvalidation_1-MAP@5:-0.19572\n",
      "[244]\tvalidation_0-merror:0.66613\tvalidation_1-merror:0.77430\tvalidation_0-MAP@5:-0.25781\tvalidation_1-MAP@5:-0.19560\n",
      "[245]\tvalidation_0-merror:0.66604\tvalidation_1-merror:0.77460\tvalidation_0-MAP@5:-0.25794\tvalidation_1-MAP@5:-0.19546\n",
      "[246]\tvalidation_0-merror:0.66578\tvalidation_1-merror:0.77442\tvalidation_0-MAP@5:-0.25803\tvalidation_1-MAP@5:-0.19550\n",
      "[247]\tvalidation_0-merror:0.66559\tvalidation_1-merror:0.77412\tvalidation_0-MAP@5:-0.25812\tvalidation_1-MAP@5:-0.19553\n",
      "[248]\tvalidation_0-merror:0.66542\tvalidation_1-merror:0.77412\tvalidation_0-MAP@5:-0.25818\tvalidation_1-MAP@5:-0.19551\n",
      "[249]\tvalidation_0-merror:0.66525\tvalidation_1-merror:0.77442\tvalidation_0-MAP@5:-0.25827\tvalidation_1-MAP@5:-0.19542\n",
      "[250]\tvalidation_0-merror:0.66499\tvalidation_1-merror:0.77412\tvalidation_0-MAP@5:-0.25836\tvalidation_1-MAP@5:-0.19557\n",
      "[251]\tvalidation_0-merror:0.66501\tvalidation_1-merror:0.77436\tvalidation_0-MAP@5:-0.25840\tvalidation_1-MAP@5:-0.19554\n",
      "[252]\tvalidation_0-merror:0.66466\tvalidation_1-merror:0.77436\tvalidation_0-MAP@5:-0.25849\tvalidation_1-MAP@5:-0.19555\n",
      "[253]\tvalidation_0-merror:0.66417\tvalidation_1-merror:0.77424\tvalidation_0-MAP@5:-0.25865\tvalidation_1-MAP@5:-0.19558\n",
      "[254]\tvalidation_0-merror:0.66409\tvalidation_1-merror:0.77430\tvalidation_0-MAP@5:-0.25874\tvalidation_1-MAP@5:-0.19557\n",
      "[255]\tvalidation_0-merror:0.66385\tvalidation_1-merror:0.77424\tvalidation_0-MAP@5:-0.25888\tvalidation_1-MAP@5:-0.19561\n",
      "[256]\tvalidation_0-merror:0.66383\tvalidation_1-merror:0.77412\tvalidation_0-MAP@5:-0.25890\tvalidation_1-MAP@5:-0.19563\n",
      "[257]\tvalidation_0-merror:0.66358\tvalidation_1-merror:0.77399\tvalidation_0-MAP@5:-0.25900\tvalidation_1-MAP@5:-0.19567\n",
      "[258]\tvalidation_0-merror:0.66351\tvalidation_1-merror:0.77406\tvalidation_0-MAP@5:-0.25907\tvalidation_1-MAP@5:-0.19566\n",
      "[259]\tvalidation_0-merror:0.66339\tvalidation_1-merror:0.77412\tvalidation_0-MAP@5:-0.25916\tvalidation_1-MAP@5:-0.19567\n",
      "[260]\tvalidation_0-merror:0.66339\tvalidation_1-merror:0.77412\tvalidation_0-MAP@5:-0.25918\tvalidation_1-MAP@5:-0.19576\n",
      "[261]\tvalidation_0-merror:0.66307\tvalidation_1-merror:0.77393\tvalidation_0-MAP@5:-0.25935\tvalidation_1-MAP@5:-0.19574\n",
      "[262]\tvalidation_0-merror:0.66263\tvalidation_1-merror:0.77424\tvalidation_0-MAP@5:-0.25948\tvalidation_1-MAP@5:-0.19566\n",
      "[263]\tvalidation_0-merror:0.66237\tvalidation_1-merror:0.77399\tvalidation_0-MAP@5:-0.25963\tvalidation_1-MAP@5:-0.19574\n",
      "[264]\tvalidation_0-merror:0.66202\tvalidation_1-merror:0.77430\tvalidation_0-MAP@5:-0.25972\tvalidation_1-MAP@5:-0.19568\n",
      "[265]\tvalidation_0-merror:0.66161\tvalidation_1-merror:0.77418\tvalidation_0-MAP@5:-0.25987\tvalidation_1-MAP@5:-0.19572\n",
      "[266]\tvalidation_0-merror:0.66149\tvalidation_1-merror:0.77399\tvalidation_0-MAP@5:-0.25994\tvalidation_1-MAP@5:-0.19574\n",
      "[267]\tvalidation_0-merror:0.66135\tvalidation_1-merror:0.77399\tvalidation_0-MAP@5:-0.26003\tvalidation_1-MAP@5:-0.19576\n",
      "[268]\tvalidation_0-merror:0.66120\tvalidation_1-merror:0.77381\tvalidation_0-MAP@5:-0.26010\tvalidation_1-MAP@5:-0.19572\n",
      "[269]\tvalidation_0-merror:0.66114\tvalidation_1-merror:0.77369\tvalidation_0-MAP@5:-0.26019\tvalidation_1-MAP@5:-0.19573\n",
      "[270]\tvalidation_0-merror:0.66077\tvalidation_1-merror:0.77412\tvalidation_0-MAP@5:-0.26032\tvalidation_1-MAP@5:-0.19561\n",
      "[271]\tvalidation_0-merror:0.66076\tvalidation_1-merror:0.77375\tvalidation_0-MAP@5:-0.26040\tvalidation_1-MAP@5:-0.19572\n",
      "[272]\tvalidation_0-merror:0.66058\tvalidation_1-merror:0.77387\tvalidation_0-MAP@5:-0.26045\tvalidation_1-MAP@5:-0.19566\n",
      "[273]\tvalidation_0-merror:0.66029\tvalidation_1-merror:0.77381\tvalidation_0-MAP@5:-0.26061\tvalidation_1-MAP@5:-0.19568\n",
      "[274]\tvalidation_0-merror:0.66009\tvalidation_1-merror:0.77375\tvalidation_0-MAP@5:-0.26068\tvalidation_1-MAP@5:-0.19569\n",
      "[275]\tvalidation_0-merror:0.65985\tvalidation_1-merror:0.77345\tvalidation_0-MAP@5:-0.26079\tvalidation_1-MAP@5:-0.19576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[276]\tvalidation_0-merror:0.65957\tvalidation_1-merror:0.77351\tvalidation_0-MAP@5:-0.26090\tvalidation_1-MAP@5:-0.19578\n",
      "[277]\tvalidation_0-merror:0.65925\tvalidation_1-merror:0.77351\tvalidation_0-MAP@5:-0.26100\tvalidation_1-MAP@5:-0.19575\n",
      "[278]\tvalidation_0-merror:0.65940\tvalidation_1-merror:0.77302\tvalidation_0-MAP@5:-0.26105\tvalidation_1-MAP@5:-0.19593\n",
      "[279]\tvalidation_0-merror:0.65910\tvalidation_1-merror:0.77320\tvalidation_0-MAP@5:-0.26118\tvalidation_1-MAP@5:-0.19596\n",
      "[280]\tvalidation_0-merror:0.65886\tvalidation_1-merror:0.77314\tvalidation_0-MAP@5:-0.26129\tvalidation_1-MAP@5:-0.19598\n",
      "[281]\tvalidation_0-merror:0.65861\tvalidation_1-merror:0.77308\tvalidation_0-MAP@5:-0.26138\tvalidation_1-MAP@5:-0.19604\n",
      "[282]\tvalidation_0-merror:0.65832\tvalidation_1-merror:0.77314\tvalidation_0-MAP@5:-0.26146\tvalidation_1-MAP@5:-0.19607\n",
      "[283]\tvalidation_0-merror:0.65822\tvalidation_1-merror:0.77314\tvalidation_0-MAP@5:-0.26155\tvalidation_1-MAP@5:-0.19604\n",
      "[284]\tvalidation_0-merror:0.65822\tvalidation_1-merror:0.77326\tvalidation_0-MAP@5:-0.26164\tvalidation_1-MAP@5:-0.19607\n",
      "[285]\tvalidation_0-merror:0.65803\tvalidation_1-merror:0.77326\tvalidation_0-MAP@5:-0.26177\tvalidation_1-MAP@5:-0.19607\n",
      "[286]\tvalidation_0-merror:0.65787\tvalidation_1-merror:0.77314\tvalidation_0-MAP@5:-0.26188\tvalidation_1-MAP@5:-0.19612\n",
      "[287]\tvalidation_0-merror:0.65776\tvalidation_1-merror:0.77308\tvalidation_0-MAP@5:-0.26195\tvalidation_1-MAP@5:-0.19611\n",
      "[288]\tvalidation_0-merror:0.65730\tvalidation_1-merror:0.77290\tvalidation_0-MAP@5:-0.26208\tvalidation_1-MAP@5:-0.19616\n",
      "[289]\tvalidation_0-merror:0.65729\tvalidation_1-merror:0.77284\tvalidation_0-MAP@5:-0.26220\tvalidation_1-MAP@5:-0.19614\n",
      "[290]\tvalidation_0-merror:0.65716\tvalidation_1-merror:0.77272\tvalidation_0-MAP@5:-0.26226\tvalidation_1-MAP@5:-0.19611\n",
      "[291]\tvalidation_0-merror:0.65694\tvalidation_1-merror:0.77241\tvalidation_0-MAP@5:-0.26240\tvalidation_1-MAP@5:-0.19627\n",
      "[292]\tvalidation_0-merror:0.65672\tvalidation_1-merror:0.77259\tvalidation_0-MAP@5:-0.26255\tvalidation_1-MAP@5:-0.19620\n",
      "[293]\tvalidation_0-merror:0.65672\tvalidation_1-merror:0.77278\tvalidation_0-MAP@5:-0.26260\tvalidation_1-MAP@5:-0.19617\n",
      "[294]\tvalidation_0-merror:0.65634\tvalidation_1-merror:0.77241\tvalidation_0-MAP@5:-0.26272\tvalidation_1-MAP@5:-0.19614\n",
      "[295]\tvalidation_0-merror:0.65621\tvalidation_1-merror:0.77247\tvalidation_0-MAP@5:-0.26280\tvalidation_1-MAP@5:-0.19613\n",
      "[296]\tvalidation_0-merror:0.65584\tvalidation_1-merror:0.77241\tvalidation_0-MAP@5:-0.26295\tvalidation_1-MAP@5:-0.19611\n",
      "[297]\tvalidation_0-merror:0.65563\tvalidation_1-merror:0.77265\tvalidation_0-MAP@5:-0.26304\tvalidation_1-MAP@5:-0.19614\n",
      "[298]\tvalidation_0-merror:0.65541\tvalidation_1-merror:0.77253\tvalidation_0-MAP@5:-0.26310\tvalidation_1-MAP@5:-0.19621\n",
      "[299]\tvalidation_0-merror:0.65505\tvalidation_1-merror:0.77241\tvalidation_0-MAP@5:-0.26322\tvalidation_1-MAP@5:-0.19624\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n",
      "Error: name 'agg1' is not defined\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(train, y, stratify=y, test_size=0.2) # stratify 保证各个类样本的均衡性\n",
    "# eval_metric 观测指标， early_stopping_rounds 早停次数\n",
    "clf.fit(X_train, y_train, early_stopping_rounds=50, eval_metric=map5eval, eval_set=[(X_train, y_train),(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 rows completed\n",
      "20000 rows completed\n",
      "30000 rows completed\n",
      "40000 rows completed\n",
      "50000 rows completed\n",
      "60000 rows completed\n",
      "70000 rows completed\n",
      "80000 rows completed\n",
      "90000 rows completed\n",
      "100000 rows completed\n",
      "110000 rows completed\n",
      "120000 rows completed\n",
      "130000 rows completed\n",
      "140000 rows completed\n",
      "150000 rows completed\n",
      "160000 rows completed\n",
      "170000 rows completed\n",
      "180000 rows completed\n",
      "190000 rows completed\n",
      "200000 rows completed\n",
      "210000 rows completed\n",
      "220000 rows completed\n",
      "230000 rows completed\n",
      "240000 rows completed\n",
      "250000 rows completed\n",
      "260000 rows completed\n",
      "270000 rows completed\n",
      "280000 rows completed\n",
      "290000 rows completed\n",
      "300000 rows completed\n",
      "310000 rows completed\n",
      "320000 rows completed\n",
      "330000 rows completed\n",
      "340000 rows completed\n",
      "350000 rows completed\n",
      "360000 rows completed\n",
      "370000 rows completed\n",
      "380000 rows completed\n",
      "390000 rows completed\n",
      "400000 rows completed\n",
      "410000 rows completed\n",
      "420000 rows completed\n",
      "430000 rows completed\n",
      "440000 rows completed\n",
      "450000 rows completed\n",
      "460000 rows completed\n",
      "470000 rows completed\n",
      "480000 rows completed\n",
      "490000 rows completed\n",
      "500000 rows completed\n",
      "510000 rows completed\n",
      "520000 rows completed\n",
      "530000 rows completed\n",
      "540000 rows completed\n",
      "550000 rows completed\n",
      "560000 rows completed\n",
      "570000 rows completed\n",
      "580000 rows completed\n",
      "590000 rows completed\n",
      "600000 rows completed\n",
      "610000 rows completed\n",
      "620000 rows completed\n",
      "630000 rows completed\n",
      "640000 rows completed\n",
      "650000 rows completed\n",
      "660000 rows completed\n",
      "670000 rows completed\n",
      "680000 rows completed\n",
      "690000 rows completed\n",
      "700000 rows completed\n",
      "710000 rows completed\n",
      "720000 rows completed\n",
      "730000 rows completed\n",
      "740000 rows completed\n",
      "750000 rows completed\n",
      "760000 rows completed\n",
      "770000 rows completed\n",
      "780000 rows completed\n",
      "790000 rows completed\n",
      "800000 rows completed\n",
      "810000 rows completed\n",
      "820000 rows completed\n",
      "830000 rows completed\n",
      "840000 rows completed\n",
      "850000 rows completed\n",
      "860000 rows completed\n",
      "870000 rows completed\n",
      "880000 rows completed\n",
      "890000 rows completed\n",
      "900000 rows completed\n",
      "910000 rows completed\n",
      "920000 rows completed\n",
      "930000 rows completed\n",
      "940000 rows completed\n",
      "950000 rows completed\n",
      "960000 rows completed\n",
      "970000 rows completed\n",
      "980000 rows completed\n",
      "990000 rows completed\n",
      "1000000 rows completed\n",
      "1010000 rows completed\n",
      "1020000 rows completed\n",
      "1030000 rows completed\n",
      "1040000 rows completed\n",
      "1050000 rows completed\n",
      "1060000 rows completed\n",
      "1070000 rows completed\n",
      "1080000 rows completed\n",
      "1090000 rows completed\n",
      "1100000 rows completed\n",
      "1110000 rows completed\n",
      "1120000 rows completed\n",
      "1130000 rows completed\n",
      "1140000 rows completed\n",
      "1150000 rows completed\n",
      "1160000 rows completed\n",
      "1170000 rows completed\n",
      "1180000 rows completed\n",
      "1190000 rows completed\n",
      "1200000 rows completed\n",
      "1210000 rows completed\n",
      "1220000 rows completed\n",
      "1230000 rows completed\n",
      "1240000 rows completed\n",
      "1250000 rows completed\n",
      "1260000 rows completed\n",
      "1270000 rows completed\n",
      "1280000 rows completed\n",
      "1290000 rows completed\n",
      "1300000 rows completed\n",
      "1310000 rows completed\n",
      "1320000 rows completed\n",
      "1330000 rows completed\n",
      "1340000 rows completed\n",
      "1350000 rows completed\n",
      "1360000 rows completed\n",
      "1370000 rows completed\n",
      "1380000 rows completed\n",
      "1390000 rows completed\n",
      "1400000 rows completed\n",
      "1410000 rows completed\n",
      "1420000 rows completed\n",
      "1430000 rows completed\n",
      "1440000 rows completed\n",
      "1450000 rows completed\n",
      "1460000 rows completed\n",
      "1470000 rows completed\n",
      "1480000 rows completed\n",
      "1490000 rows completed\n",
      "1500000 rows completed\n",
      "1510000 rows completed\n",
      "1520000 rows completed\n",
      "1530000 rows completed\n",
      "1540000 rows completed\n",
      "1550000 rows completed\n",
      "1560000 rows completed\n",
      "1570000 rows completed\n",
      "1580000 rows completed\n",
      "1590000 rows completed\n",
      "1600000 rows completed\n",
      "1610000 rows completed\n",
      "1620000 rows completed\n",
      "1630000 rows completed\n",
      "1640000 rows completed\n",
      "1650000 rows completed\n",
      "1660000 rows completed\n",
      "1670000 rows completed\n",
      "1680000 rows completed\n",
      "1690000 rows completed\n",
      "1700000 rows completed\n",
      "1710000 rows completed\n",
      "1720000 rows completed\n",
      "1730000 rows completed\n",
      "1740000 rows completed\n",
      "1750000 rows completed\n",
      "1760000 rows completed\n",
      "1770000 rows completed\n",
      "1780000 rows completed\n",
      "1790000 rows completed\n",
      "1800000 rows completed\n",
      "1810000 rows completed\n",
      "1820000 rows completed\n",
      "1830000 rows completed\n",
      "1840000 rows completed\n",
      "1850000 rows completed\n",
      "1860000 rows completed\n",
      "1870000 rows completed\n",
      "1880000 rows completed\n",
      "1890000 rows completed\n",
      "1900000 rows completed\n",
      "1910000 rows completed\n",
      "1920000 rows completed\n",
      "1930000 rows completed\n",
      "1940000 rows completed\n",
      "1950000 rows completed\n",
      "1960000 rows completed\n",
      "1970000 rows completed\n",
      "1980000 rows completed\n",
      "1990000 rows completed\n",
      "2000000 rows completed\n",
      "2010000 rows completed\n",
      "2020000 rows completed\n",
      "2030000 rows completed\n",
      "2040000 rows completed\n",
      "2050000 rows completed\n",
      "2060000 rows completed\n",
      "2070000 rows completed\n",
      "2080000 rows completed\n",
      "2090000 rows completed\n",
      "2100000 rows completed\n",
      "2110000 rows completed\n",
      "2120000 rows completed\n",
      "2130000 rows completed\n",
      "2140000 rows completed\n",
      "2150000 rows completed\n",
      "2160000 rows completed\n",
      "2170000 rows completed\n",
      "2180000 rows completed\n",
      "2190000 rows completed\n",
      "2200000 rows completed\n",
      "2210000 rows completed\n",
      "2220000 rows completed\n",
      "2230000 rows completed\n",
      "2240000 rows completed\n",
      "2250000 rows completed\n",
      "2260000 rows completed\n",
      "2270000 rows completed\n",
      "2280000 rows completed\n",
      "2290000 rows completed\n",
      "2300000 rows completed\n",
      "2310000 rows completed\n",
      "2320000 rows completed\n",
      "2330000 rows completed\n",
      "2340000 rows completed\n",
      "2350000 rows completed\n",
      "2360000 rows completed\n",
      "2370000 rows completed\n",
      "2380000 rows completed\n",
      "2390000 rows completed\n",
      "2400000 rows completed\n",
      "2410000 rows completed\n",
      "2420000 rows completed\n",
      "2430000 rows completed\n",
      "2440000 rows completed\n",
      "2450000 rows completed\n",
      "2460000 rows completed\n",
      "2470000 rows completed\n",
      "2480000 rows completed\n",
      "2490000 rows completed\n",
      "2500000 rows completed\n",
      "2510000 rows completed\n",
      "2520000 rows completed\n",
      "2530000 rows completed\n"
     ]
    }
   ],
   "source": [
    "# prediction process\n",
    "count = 0\n",
    "chunksize = 10000\n",
    "preds = np.empty((submission.shape[0],clf.n_classes_))\n",
    "reader = pd.read_csv('./output/test.csv', parse_dates=['date_time', 'srch_ci', 'srch_co'], chunksize=chunksize)\n",
    "for chunk in reader:\n",
    "    try:\n",
    "        chunk = pd.merge(chunk, destinations, how='left', on='srch_destination_id')\n",
    "        chunk = pd.merge(chunk, aggMod, how='left', on=['srch_destination_id','hotel_country','hotel_market'])\n",
    "        chunk.drop(['id'], axis=1, inplace=True)\n",
    "        pre_process(chunk)\n",
    "\n",
    "        pred = clf.predict_proba(chunk)\n",
    "        preds[count:(count + chunk.shape[0]),:] = pred\n",
    "        count = count + chunksize\n",
    "        print('%d rows completed' % count)\n",
    "    except Exception as e:\n",
    "        print('Error: %s' % str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output predict result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing latest probabilities to file\n"
     ]
    }
   ],
   "source": [
    "del clf\n",
    "if os.path.exists('./output/probs/allpreds_xgb.h5'):\n",
    "    with h5py.File('./output/probs/allpreds_xgb.h5', 'r+') as hf:\n",
    "        print('reading in and combining probabilities')\n",
    "        predshf = hf['preds']\n",
    "        preds += predshf.value\n",
    "        print('writing latest probabilities to file')\n",
    "        predshf[...] = preds\n",
    "else:\n",
    "    with h5py.File('./output/probs/allpreds_xgb.h5', 'w') as hf:\n",
    "        print('writing latest probabilities to file')\n",
    "        hf.create_dataset('preds', data=preds)\n",
    "\n",
    "# print('generating submission')\n",
    "# col_ind = np.argsort(-preds, axis=1)[:,:5] # 取出最大的5个\n",
    "# hc = [' '.join(row.astype(str)) for row in col_ind]\n",
    "# sub = pd.DataFrame(data=hc, index=submission.id)\n",
    "# sub.reset_index(inplace=True)\n",
    "# sub.columns = submission.columns\n",
    "# sub.to_csv('./output/pred_sub.csv', index=False)\n",
    "\n",
    "\n",
    "skipsize += tchunksize\n",
    "with open('rows_complete.txt', 'w') as f:\n",
    "    f.write(str(skipsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD分类建模SGD classifier modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 做类别型变量的处理，编码 categorical attributes process and encode\n",
    "cat_col = ['user_id', 'user_location_city',\n",
    "           'srch_destination_id', 'srch_destination_type_id', 'hotel_continent',\n",
    "           'hotel_country', 'hotel_market']\n",
    "\n",
    "num_col = ['is_mobile', 'is_package']\n",
    "\n",
    "# 时间分箱 time binning\n",
    "def bin_time(t):\n",
    "    if t < 0:\n",
    "        x = 0\n",
    "    elif t < 2:\n",
    "        x = 1\n",
    "    elif t < 7:\n",
    "        x = 2\n",
    "    elif t < 30:\n",
    "        x = 3\n",
    "    else:\n",
    "        x = 4    \n",
    "    return x\n",
    "\n",
    "def pre_processSGD(data):\n",
    "    \n",
    "    data['ci_month'] = data['srch_ci'].apply(lambda dt: dt.month)\n",
    "    data['season_dest'] = 'season_dest' + data.ci_month.map(str) + '*' + data.srch_destination_id.map(str)\n",
    "    data['season_dest'] = data['season_dest'].map(hash)\n",
    "    data['time_to_ci'] = data.srch_ci-data.date_time\n",
    "    data['time_to_ci'] = data['time_to_ci'].apply(lambda td: td/np.timedelta64(1, 'D'))\n",
    "    data['time_to_ci'] = data['time_to_ci'].map(bin_time)\n",
    "    data['time_dest'] = 'time_dest' + data.time_to_ci.map(str) + '*' + data.srch_destination_id.map(str)\n",
    "    data['time_dest'] = data['time_dest'].map(hash)\n",
    "    data.fillna(0, inplace=True)\n",
    "    \n",
    "    # 类别型变量做哈希 hash categorical attributes \n",
    "    for col in cat_col:\n",
    "        data[col] = col + data[col].map(str)\n",
    "        data[col] = data[col].map(hash)\n",
    "\n",
    "cat_col_all = cat_col + ['season_dest', 'time_dest'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('./output/probs/sgd.pkl'):\n",
    "    with open('./output/probs/sgd.pkl', 'rb') as f:\n",
    "        clf = pickle.load(f)\n",
    "else:\n",
    "    clf = SGDClassifier(loss='log', n_jobs=-1, alpha=0.0000025, verbose=0) # loss Softmax\n",
    "    \n",
    "# for epoch in range(5):\n",
    "count = 0\n",
    "chunksize = 200000\n",
    "n_features = 3000000\n",
    "print('Epoch %d started' % epoch)\n",
    "reader = pd.read_csv('./output/train.csv', parse_dates=['date_time', 'srch_ci', 'srch_co'], chunksize=chunksize)\n",
    "for chunk in reader:\n",
    "    try:\n",
    "        pre_processSGD(chunk)\n",
    "        y = chunk.hotel_cluster\n",
    "        sw = 1 + 4*chunk.is_booking # 加重booking权重\n",
    "        chunk.drop(['cnt', 'hotel_cluster', 'is_booking'], axis=1, inplace=True) # 删除不需要的特征\n",
    "\n",
    "        # 稀疏化处理\n",
    "        XN = csr_matrix(chunk[num_col].values)\n",
    "        X = csr_matrix((chunk.shape[0], n_features))\n",
    "        rows = np.arange(chunk.shape[0])\n",
    "        for col in cat_col_all:\n",
    "            dat = np.ones(chunk.shape[0])\n",
    "            cols = chunk[col] % n_features\n",
    "            X += csr_matrix((dat, (rows, cols)), shape=(chunk.shape[0], n_features))\n",
    "        X = hstack((XN, X)) # 拼接数据\n",
    "    #         book_indices = sw[sw > 1].index.tolist()\n",
    "    #         X_test = csr_matrix(X)[book_indices]\n",
    "    #         y_test = y[book_indices]\n",
    "\n",
    "        clf.partial_fit(X, y, classes=np.arange(100), sample_weight=sw)\n",
    "\n",
    "        count = count + chunksize\n",
    "    #             map5 = map5eval(clf.predict_proba(X_test), y_test)\n",
    "    #             print('%d rows completed. MAP@5: %f' % (count, map5))\n",
    "        print('%d rows completed' % count)\n",
    "        if(count/chunksize == 200):\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print('Error: %s' % str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 rows completed\n",
      "20000 rows completed\n",
      "30000 rows completed\n",
      "40000 rows completed\n",
      "50000 rows completed\n",
      "60000 rows completed\n",
      "70000 rows completed\n",
      "80000 rows completed\n",
      "90000 rows completed\n",
      "100000 rows completed\n",
      "110000 rows completed\n",
      "120000 rows completed\n",
      "130000 rows completed\n",
      "140000 rows completed\n",
      "150000 rows completed\n",
      "160000 rows completed\n",
      "170000 rows completed\n",
      "180000 rows completed\n",
      "190000 rows completed\n",
      "200000 rows completed\n",
      "210000 rows completed\n",
      "220000 rows completed\n",
      "230000 rows completed\n",
      "240000 rows completed\n",
      "250000 rows completed\n",
      "260000 rows completed\n",
      "270000 rows completed\n",
      "280000 rows completed\n",
      "290000 rows completed\n",
      "300000 rows completed\n",
      "310000 rows completed\n",
      "320000 rows completed\n",
      "330000 rows completed\n",
      "340000 rows completed\n",
      "350000 rows completed\n",
      "360000 rows completed\n",
      "370000 rows completed\n",
      "380000 rows completed\n",
      "390000 rows completed\n",
      "400000 rows completed\n",
      "410000 rows completed\n",
      "420000 rows completed\n",
      "430000 rows completed\n",
      "440000 rows completed\n",
      "450000 rows completed\n",
      "460000 rows completed\n",
      "470000 rows completed\n",
      "480000 rows completed\n",
      "490000 rows completed\n",
      "500000 rows completed\n",
      "510000 rows completed\n",
      "520000 rows completed\n",
      "530000 rows completed\n",
      "540000 rows completed\n",
      "550000 rows completed\n",
      "560000 rows completed\n",
      "570000 rows completed\n",
      "580000 rows completed\n",
      "590000 rows completed\n",
      "600000 rows completed\n",
      "610000 rows completed\n",
      "620000 rows completed\n",
      "630000 rows completed\n",
      "640000 rows completed\n",
      "650000 rows completed\n",
      "660000 rows completed\n",
      "670000 rows completed\n",
      "680000 rows completed\n",
      "690000 rows completed\n",
      "700000 rows completed\n",
      "710000 rows completed\n",
      "720000 rows completed\n",
      "730000 rows completed\n",
      "740000 rows completed\n",
      "750000 rows completed\n",
      "760000 rows completed\n",
      "770000 rows completed\n",
      "780000 rows completed\n",
      "790000 rows completed\n",
      "800000 rows completed\n",
      "810000 rows completed\n",
      "820000 rows completed\n",
      "830000 rows completed\n",
      "840000 rows completed\n",
      "850000 rows completed\n",
      "860000 rows completed\n",
      "870000 rows completed\n",
      "880000 rows completed\n",
      "890000 rows completed\n",
      "900000 rows completed\n",
      "910000 rows completed\n",
      "920000 rows completed\n",
      "930000 rows completed\n",
      "940000 rows completed\n",
      "950000 rows completed\n",
      "960000 rows completed\n",
      "970000 rows completed\n",
      "980000 rows completed\n",
      "990000 rows completed\n",
      "1000000 rows completed\n",
      "1010000 rows completed\n",
      "1020000 rows completed\n",
      "1030000 rows completed\n",
      "1040000 rows completed\n",
      "1050000 rows completed\n",
      "1060000 rows completed\n",
      "1070000 rows completed\n",
      "1080000 rows completed\n",
      "1090000 rows completed\n",
      "1100000 rows completed\n",
      "1110000 rows completed\n",
      "1120000 rows completed\n",
      "1130000 rows completed\n",
      "1140000 rows completed\n",
      "1150000 rows completed\n",
      "1160000 rows completed\n",
      "1170000 rows completed\n",
      "1180000 rows completed\n",
      "1190000 rows completed\n",
      "1200000 rows completed\n",
      "1210000 rows completed\n",
      "1220000 rows completed\n",
      "1230000 rows completed\n",
      "1240000 rows completed\n",
      "1250000 rows completed\n",
      "1260000 rows completed\n",
      "1270000 rows completed\n",
      "1280000 rows completed\n",
      "1290000 rows completed\n",
      "1300000 rows completed\n",
      "1310000 rows completed\n",
      "1320000 rows completed\n",
      "1330000 rows completed\n",
      "1340000 rows completed\n",
      "1350000 rows completed\n",
      "1360000 rows completed\n",
      "1370000 rows completed\n",
      "1380000 rows completed\n",
      "1390000 rows completed\n",
      "1400000 rows completed\n",
      "1410000 rows completed\n",
      "1420000 rows completed\n",
      "1430000 rows completed\n",
      "1440000 rows completed\n",
      "1450000 rows completed\n",
      "1460000 rows completed\n",
      "1470000 rows completed\n",
      "1480000 rows completed\n",
      "1490000 rows completed\n",
      "1500000 rows completed\n",
      "1510000 rows completed\n",
      "1520000 rows completed\n",
      "1530000 rows completed\n",
      "1540000 rows completed\n",
      "1550000 rows completed\n",
      "1560000 rows completed\n",
      "1570000 rows completed\n",
      "1580000 rows completed\n",
      "1590000 rows completed\n",
      "1600000 rows completed\n",
      "1610000 rows completed\n",
      "1620000 rows completed\n",
      "1630000 rows completed\n",
      "1640000 rows completed\n",
      "1650000 rows completed\n",
      "1660000 rows completed\n",
      "1670000 rows completed\n",
      "1680000 rows completed\n",
      "1690000 rows completed\n",
      "1700000 rows completed\n",
      "1710000 rows completed\n",
      "1720000 rows completed\n",
      "1730000 rows completed\n",
      "1740000 rows completed\n",
      "1750000 rows completed\n",
      "1760000 rows completed\n",
      "1770000 rows completed\n",
      "1780000 rows completed\n",
      "1790000 rows completed\n",
      "1800000 rows completed\n",
      "1810000 rows completed\n",
      "1820000 rows completed\n",
      "1830000 rows completed\n",
      "1840000 rows completed\n",
      "1850000 rows completed\n",
      "1860000 rows completed\n",
      "1870000 rows completed\n",
      "1880000 rows completed\n",
      "1890000 rows completed\n",
      "1900000 rows completed\n",
      "1910000 rows completed\n",
      "1920000 rows completed\n",
      "1930000 rows completed\n",
      "1940000 rows completed\n",
      "1950000 rows completed\n",
      "1960000 rows completed\n",
      "1970000 rows completed\n",
      "1980000 rows completed\n",
      "1990000 rows completed\n",
      "2000000 rows completed\n",
      "2010000 rows completed\n",
      "2020000 rows completed\n",
      "2030000 rows completed\n",
      "2040000 rows completed\n",
      "2050000 rows completed\n",
      "2060000 rows completed\n",
      "2070000 rows completed\n",
      "2080000 rows completed\n",
      "2090000 rows completed\n",
      "2100000 rows completed\n",
      "2110000 rows completed\n",
      "2120000 rows completed\n",
      "2130000 rows completed\n",
      "2140000 rows completed\n",
      "2150000 rows completed\n",
      "2160000 rows completed\n",
      "2170000 rows completed\n",
      "2180000 rows completed\n",
      "2190000 rows completed\n",
      "2200000 rows completed\n",
      "2210000 rows completed\n",
      "2220000 rows completed\n",
      "2230000 rows completed\n",
      "2240000 rows completed\n",
      "2250000 rows completed\n",
      "2260000 rows completed\n",
      "2270000 rows completed\n",
      "2280000 rows completed\n",
      "2290000 rows completed\n",
      "2300000 rows completed\n",
      "2310000 rows completed\n",
      "2320000 rows completed\n",
      "2330000 rows completed\n",
      "2340000 rows completed\n",
      "2350000 rows completed\n",
      "2360000 rows completed\n",
      "2370000 rows completed\n",
      "2380000 rows completed\n",
      "2390000 rows completed\n",
      "2400000 rows completed\n",
      "2410000 rows completed\n",
      "2420000 rows completed\n",
      "2430000 rows completed\n",
      "2440000 rows completed\n",
      "2450000 rows completed\n",
      "2460000 rows completed\n",
      "2470000 rows completed\n",
      "2480000 rows completed\n",
      "2490000 rows completed\n",
      "2500000 rows completed\n",
      "2510000 rows completed\n",
      "2520000 rows completed\n",
      "2530000 rows completed\n"
     ]
    }
   ],
   "source": [
    "# prediction process\n",
    "with open('./output/probs/sgd.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "\n",
    "count = 0\n",
    "chunksize = 10000\n",
    "preds = np.empty((0,100))\n",
    "reader = pd.read_csv('./output/test.csv', parse_dates=['date_time', 'srch_ci', 'srch_co'], chunksize=chunksize)\n",
    "for chunk in reader:\n",
    "    #chunk = pd.merge(chunk, destinations, how='left', on='srch_destination_id')\n",
    "    #chunk = pd.merge(chunk, agg1, how='left', on='srch_destination_id')\n",
    "    chunk.drop(['id'], axis=1, inplace=True)\n",
    "    pre_processSGD(chunk)\n",
    "    \n",
    "    XN = csr_matrix(chunk[num_col].values)\n",
    "    X = csr_matrix((chunk.shape[0], n_features))\n",
    "    rows = np.arange(chunk.shape[0])\n",
    "    for col in cat_col_all:\n",
    "        dat = np.ones(chunk.shape[0])\n",
    "        cols = chunk[col] % n_features\n",
    "        X += csr_matrix((dat, (rows, cols)), shape=(chunk.shape[0], n_features))\n",
    "    X = hstack((XN, X))\n",
    "    \n",
    "    pred = clf.predict_proba(X)\n",
    "    preds = np.vstack((preds, pred))\n",
    "    count = count + chunksize\n",
    "    print('%d rows completed' % count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing latest probabilities to file\n"
     ]
    }
   ],
   "source": [
    "del clf\n",
    "del reader\n",
    "# 存储结果\n",
    "if os.path.exists('./output/probs/allpreds_sgd.h5'):\n",
    "    with h5py.File('./output/probs/allpreds_sgd.h5', 'r+') as hf:\n",
    "        #print('reading in and combining probabilities')\n",
    "        predshf = hf['preds']\n",
    "        preds += predshf.value\n",
    "        print('writing latest probabilities to file')\n",
    "        predshf[...] = preds\n",
    "else:\n",
    "    with h5py.File('./output/probs/allpreds_sgd.h5', 'w') as hf:\n",
    "        print('writing latest probabilities to file')\n",
    "        hf.create_dataset('preds', data=preds)\n",
    "\n",
    "# col_ind = np.argsort(-preds, axis=1)[:,:5]\n",
    "# hc = [' '.join(row.astype(str)) for row in col_ind]\n",
    "\n",
    "# sub = pd.DataFrame(data=hc, index=submission.id)\n",
    "# sub.reset_index(inplace=True)\n",
    "# sub.columns = submission.columns\n",
    "# sub.to_csv('./output/pred_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯建模Naive Bayes modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('./output/probs/bnb.pkl'):\n",
    "    with open('./output/probs/bnb.pkl', 'rb') as f:\n",
    "        clf = pickle.load(f)\n",
    "else:\n",
    "    clf = BernoulliNB(alpha=1.0)\n",
    "#clf.sparsify()\n",
    "# for epoch in range(1):\n",
    "count = 0\n",
    "chunksize = 200000\n",
    "n_features = 1000000\n",
    "print('Epoch %d started' % epoch)\n",
    "reader = pd.read_csv('./output/train.csv', parse_dates=['date_time', 'srch_ci', 'srch_co'], chunksize=chunksize)\n",
    "for chunk in reader:\n",
    "    try:\n",
    "        #chunk = chunk[chunk.is_booking==1]\n",
    "        #chunk = pd.merge(chunk, destinations, how='left', on='srch_destination_id')\n",
    "        #chunk = pd.merge(chunk, agg1, how='left', on='srch_destination_id')\n",
    "        pre_processSGD(chunk)\n",
    "        #chunk = chunk[chunk.ci_year==2014]\n",
    "        y = chunk.hotel_cluster\n",
    "        sw = 1 + 4*chunk.is_booking\n",
    "        chunk.drop(['cnt', 'hotel_cluster', 'is_booking'], axis=1, inplace=True)\n",
    "\n",
    "        XN = csr_matrix(chunk[num_col].values)\n",
    "        X = csr_matrix((chunk.shape[0], n_features))\n",
    "        rows = np.arange(chunk.shape[0])\n",
    "        for col in cat_col_all:\n",
    "            dat = np.ones(chunk.shape[0])\n",
    "            cols = chunk[col] % n_features\n",
    "            X += csr_matrix((dat, (rows, cols)), shape=(chunk.shape[0], n_features))\n",
    "        X = hstack((XN, X))\n",
    "#         book_indices = sw[sw > 1].index.tolist()\n",
    "#         X_test = csr_matrix(X)[book_indices]\n",
    "#         y_test = y[book_indices]\n",
    "\n",
    "        clf.partial_fit(X, y, classes=np.arange(100), sample_weight=sw)\n",
    "\n",
    "        count = count + chunksize\n",
    "#             map5 = map5eval(clf.predict_proba(X_test), y_test)\n",
    "#             print('%d rows completed. MAP@5: %f' % (count, map5)) # 评估结果\n",
    "        if(count/chunksize == 200):\n",
    "            break\n",
    "    except Exception as e:\n",
    "        #e = sys.exc_info()[0]\n",
    "        print('Error: %s' % str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 rows completed\n",
      "20000 rows completed\n",
      "30000 rows completed\n",
      "40000 rows completed\n",
      "50000 rows completed\n",
      "60000 rows completed\n",
      "70000 rows completed\n",
      "80000 rows completed\n",
      "90000 rows completed\n",
      "100000 rows completed\n",
      "110000 rows completed\n",
      "120000 rows completed\n",
      "130000 rows completed\n",
      "140000 rows completed\n",
      "150000 rows completed\n",
      "160000 rows completed\n",
      "170000 rows completed\n",
      "180000 rows completed\n",
      "190000 rows completed\n",
      "200000 rows completed\n",
      "210000 rows completed\n",
      "220000 rows completed\n",
      "230000 rows completed\n",
      "240000 rows completed\n",
      "250000 rows completed\n",
      "260000 rows completed\n",
      "270000 rows completed\n",
      "280000 rows completed\n",
      "290000 rows completed\n",
      "300000 rows completed\n",
      "310000 rows completed\n",
      "320000 rows completed\n",
      "330000 rows completed\n",
      "340000 rows completed\n",
      "350000 rows completed\n",
      "360000 rows completed\n",
      "370000 rows completed\n",
      "380000 rows completed\n",
      "390000 rows completed\n",
      "400000 rows completed\n",
      "410000 rows completed\n",
      "420000 rows completed\n",
      "430000 rows completed\n",
      "440000 rows completed\n",
      "450000 rows completed\n",
      "460000 rows completed\n",
      "470000 rows completed\n",
      "480000 rows completed\n",
      "490000 rows completed\n",
      "500000 rows completed\n",
      "510000 rows completed\n",
      "520000 rows completed\n",
      "530000 rows completed\n",
      "540000 rows completed\n",
      "550000 rows completed\n",
      "560000 rows completed\n",
      "570000 rows completed\n",
      "580000 rows completed\n",
      "590000 rows completed\n",
      "600000 rows completed\n",
      "610000 rows completed\n",
      "620000 rows completed\n",
      "630000 rows completed\n",
      "640000 rows completed\n",
      "650000 rows completed\n",
      "660000 rows completed\n",
      "670000 rows completed\n",
      "680000 rows completed\n",
      "690000 rows completed\n",
      "700000 rows completed\n",
      "710000 rows completed\n",
      "720000 rows completed\n",
      "730000 rows completed\n",
      "740000 rows completed\n",
      "750000 rows completed\n",
      "760000 rows completed\n",
      "770000 rows completed\n",
      "780000 rows completed\n",
      "790000 rows completed\n",
      "800000 rows completed\n",
      "810000 rows completed\n",
      "820000 rows completed\n",
      "830000 rows completed\n",
      "840000 rows completed\n",
      "850000 rows completed\n",
      "860000 rows completed\n",
      "870000 rows completed\n",
      "880000 rows completed\n",
      "890000 rows completed\n",
      "900000 rows completed\n",
      "910000 rows completed\n",
      "920000 rows completed\n",
      "930000 rows completed\n",
      "940000 rows completed\n",
      "950000 rows completed\n",
      "960000 rows completed\n",
      "970000 rows completed\n",
      "980000 rows completed\n",
      "990000 rows completed\n",
      "1000000 rows completed\n",
      "1010000 rows completed\n",
      "1020000 rows completed\n",
      "1030000 rows completed\n",
      "1040000 rows completed\n",
      "1050000 rows completed\n",
      "1060000 rows completed\n",
      "1070000 rows completed\n",
      "1080000 rows completed\n",
      "1090000 rows completed\n",
      "1100000 rows completed\n",
      "1110000 rows completed\n",
      "1120000 rows completed\n",
      "1130000 rows completed\n",
      "1140000 rows completed\n",
      "1150000 rows completed\n",
      "1160000 rows completed\n",
      "1170000 rows completed\n",
      "1180000 rows completed\n",
      "1190000 rows completed\n",
      "1200000 rows completed\n",
      "1210000 rows completed\n",
      "1220000 rows completed\n",
      "1230000 rows completed\n",
      "1240000 rows completed\n",
      "1250000 rows completed\n",
      "1260000 rows completed\n",
      "1270000 rows completed\n",
      "1280000 rows completed\n",
      "1290000 rows completed\n",
      "1300000 rows completed\n",
      "1310000 rows completed\n",
      "1320000 rows completed\n",
      "1330000 rows completed\n",
      "1340000 rows completed\n",
      "1350000 rows completed\n",
      "1360000 rows completed\n",
      "1370000 rows completed\n",
      "1380000 rows completed\n",
      "1390000 rows completed\n",
      "1400000 rows completed\n",
      "1410000 rows completed\n",
      "1420000 rows completed\n",
      "1430000 rows completed\n",
      "1440000 rows completed\n",
      "1450000 rows completed\n",
      "1460000 rows completed\n",
      "1470000 rows completed\n",
      "1480000 rows completed\n",
      "1490000 rows completed\n",
      "1500000 rows completed\n",
      "1510000 rows completed\n",
      "1520000 rows completed\n",
      "1530000 rows completed\n",
      "1540000 rows completed\n",
      "1550000 rows completed\n",
      "1560000 rows completed\n",
      "1570000 rows completed\n",
      "1580000 rows completed\n",
      "1590000 rows completed\n",
      "1600000 rows completed\n",
      "1610000 rows completed\n",
      "1620000 rows completed\n",
      "1630000 rows completed\n",
      "1640000 rows completed\n",
      "1650000 rows completed\n",
      "1660000 rows completed\n",
      "1670000 rows completed\n",
      "1680000 rows completed\n",
      "1690000 rows completed\n",
      "1700000 rows completed\n",
      "1710000 rows completed\n",
      "1720000 rows completed\n",
      "1730000 rows completed\n",
      "1740000 rows completed\n",
      "1750000 rows completed\n",
      "1760000 rows completed\n",
      "1770000 rows completed\n",
      "1780000 rows completed\n",
      "1790000 rows completed\n",
      "1800000 rows completed\n",
      "1810000 rows completed\n",
      "1820000 rows completed\n",
      "1830000 rows completed\n",
      "1840000 rows completed\n",
      "1850000 rows completed\n",
      "1860000 rows completed\n",
      "1870000 rows completed\n",
      "1880000 rows completed\n",
      "1890000 rows completed\n",
      "1900000 rows completed\n",
      "1910000 rows completed\n",
      "1920000 rows completed\n",
      "1930000 rows completed\n",
      "1940000 rows completed\n",
      "1950000 rows completed\n",
      "1960000 rows completed\n",
      "1970000 rows completed\n",
      "1980000 rows completed\n",
      "1990000 rows completed\n",
      "2000000 rows completed\n",
      "2010000 rows completed\n",
      "2020000 rows completed\n",
      "2030000 rows completed\n",
      "2040000 rows completed\n",
      "2050000 rows completed\n",
      "2060000 rows completed\n",
      "2070000 rows completed\n",
      "2080000 rows completed\n",
      "2090000 rows completed\n",
      "2100000 rows completed\n",
      "2110000 rows completed\n",
      "2120000 rows completed\n",
      "2130000 rows completed\n",
      "2140000 rows completed\n",
      "2150000 rows completed\n",
      "2160000 rows completed\n",
      "2170000 rows completed\n",
      "2180000 rows completed\n",
      "2190000 rows completed\n",
      "2200000 rows completed\n",
      "2210000 rows completed\n",
      "2220000 rows completed\n",
      "2230000 rows completed\n",
      "2240000 rows completed\n",
      "2250000 rows completed\n",
      "2260000 rows completed\n",
      "2270000 rows completed\n",
      "2280000 rows completed\n",
      "2290000 rows completed\n",
      "2300000 rows completed\n",
      "2310000 rows completed\n",
      "2320000 rows completed\n",
      "2330000 rows completed\n",
      "2340000 rows completed\n",
      "2350000 rows completed\n",
      "2360000 rows completed\n",
      "2370000 rows completed\n",
      "2380000 rows completed\n",
      "2390000 rows completed\n",
      "2400000 rows completed\n",
      "2410000 rows completed\n",
      "2420000 rows completed\n",
      "2430000 rows completed\n",
      "2440000 rows completed\n",
      "2450000 rows completed\n",
      "2460000 rows completed\n",
      "2470000 rows completed\n",
      "2480000 rows completed\n",
      "2490000 rows completed\n",
      "2500000 rows completed\n",
      "2510000 rows completed\n",
      "2520000 rows completed\n",
      "2530000 rows completed\n"
     ]
    }
   ],
   "source": [
    "# prediction process\n",
    "with open('./output/probs/bnb.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "\n",
    "count = 0\n",
    "chunksize = 10000\n",
    "preds = np.empty((0,100))\n",
    "reader = pd.read_csv('./output/test.csv', parse_dates=['date_time', 'srch_ci', 'srch_co'], chunksize=chunksize)\n",
    "for chunk in reader:\n",
    "    #chunk = pd.merge(chunk, destinations, how='left', on='srch_destination_id')\n",
    "    #chunk = pd.merge(chunk, agg1, how='left', on='srch_destination_id')\n",
    "    chunk.drop(['id'], axis=1, inplace=True)\n",
    "    pre_processSGD(chunk)\n",
    "    \n",
    "    XN = csr_matrix(chunk[num_col].values)\n",
    "    X = csr_matrix((chunk.shape[0], n_features))\n",
    "    rows = np.arange(chunk.shape[0])\n",
    "    for col in cat_col_all:\n",
    "        dat = np.ones(chunk.shape[0])\n",
    "        cols = chunk[col] % n_features\n",
    "        X += csr_matrix((dat, (rows, cols)), shape=(chunk.shape[0], n_features))\n",
    "    X = hstack((XN, X))\n",
    "    \n",
    "    pred = clf.predict_proba(X)\n",
    "    preds = np.vstack((preds, pred))\n",
    "    count = count + chunksize\n",
    "    print('%d rows completed' % count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing latest probabilities to file\n"
     ]
    }
   ],
   "source": [
    "del clf\n",
    "del reader\n",
    "#输出结果\n",
    "if os.path.exists('./output/probs/allpreds_bnb.h5'):\n",
    "    with h5py.File('./output/probs/allpreds_bnb.h5', 'r+') as hf:\n",
    "        #print('reading in and combining probabilities')\n",
    "        predshf = hf['preds']\n",
    "        preds += predshf.value\n",
    "        print('writing latest probabilities to file')\n",
    "        predshf[...] = preds\n",
    "else:\n",
    "    with h5py.File('./output/probs/allpreds_bnb.h5', 'w') as hf:\n",
    "        print('writing latest probabilities to file')\n",
    "        hf.create_dataset('preds', data=preds)\n",
    "\n",
    "# col_ind = np.argsort(-preds, axis=1)[:,:5]\n",
    "# hc = [' '.join(row.astype(str)) for row in col_ind]\n",
    "\n",
    "# sub = pd.DataFrame(data=hc, index=submission.id)\n",
    "# sub.reset_index(inplace=True)\n",
    "# sub.columns = submission.columns\n",
    "# sub.to_csv('./output/pred_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型融合 stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \n",
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating submission\n"
     ]
    }
   ],
   "source": [
    "# read in RF results\n",
    "with h5py.File('./output/probs/allpreds.h5', 'r') as hf:\n",
    "        predshf = hf['preds_latest']\n",
    "        preds = 0.31*normalize(predshf.value, norm='l1', axis=1)\n",
    "\n",
    "# read in XGB results\n",
    "with h5py.File('./output/probs/allpreds_xgb.h5', 'r') as hf:\n",
    "        predshf = hf['preds']\n",
    "        preds += 0.39*normalize(predshf.value, norm='l1', axis=1)\n",
    "\n",
    "# read in SGD results\n",
    "with h5py.File('./output/probs/allpreds_sgd.h5', 'r') as hf:\n",
    "        predshf = hf['preds']\n",
    "        preds += 0.27*normalize(predshf.value, norm='l1', axis=1)\n",
    "\n",
    "# read in Bernoulli NB results\n",
    "with h5py.File('./output/probs/allpreds_bnb.h5', 'r') as hf:\n",
    "        predshf = hf['preds']\n",
    "        preds += 0.03*normalize(predshf.value, norm='l1', axis=1)\n",
    "\n",
    "print('generating submission')\n",
    "col_ind = np.argsort(-preds, axis=1)[:,:5]\n",
    "hc = [' '.join(row.astype(str)) for row in col_ind]\n",
    "\n",
    "sub = pd.DataFrame(data=hc, index=submission.id)\n",
    "sub.reset_index(inplace=True)\n",
    "sub.columns = submission.columns\n",
    "sub.to_csv('./output/pred_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提交结果格式处理 submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_pred = pd.read_csv('./output/match_pred.csv')\n",
    "match_pred.fillna('', inplace=True)\n",
    "match_pred = match_pred['hotel_cluster'].tolist()\n",
    "match_pred = [s.split(' ') for s in match_pred]\n",
    "\n",
    "pred_sub = pd.read_csv('./output/pred_sub.csv')\n",
    "ids = pred_sub.id\n",
    "pred_sub = pred_sub['hotel_cluster'].tolist()\n",
    "pred_sub = [s.split(' ') for s in pred_sub]\n",
    "\n",
    "# 取出前5\n",
    "def f5(seq, idfun=None): \n",
    "    if idfun is None:\n",
    "        def idfun(x): return x\n",
    "    seen = {}\n",
    "    result = []\n",
    "    for item in seq:\n",
    "        marker = idfun(item)\n",
    "        if (marker in seen) or (marker == ''): continue\n",
    "        seen[marker] = 1\n",
    "        result.append(item)\n",
    "    return result\n",
    "    \n",
    "full_preds = [f5(match_pred[p] + pred_sub[p])[:5] for p in range(len(pred_sub))]\n",
    "\n",
    "write_p = [\" \".join([str(l) for l in p]) for p in full_preds]\n",
    "write_frame = [\"{0},{1}\".format(ids[i], write_p[i]) for i in range(len(full_preds))]\n",
    "write_frame = [\"id,hotel_cluster\"] + write_frame\n",
    "with open(\"./output/predictions.csv\", \"w+\") as f:\n",
    "    f.write(\"\\n\".join(write_frame))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
